{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autotime\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hetinf_pmd\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "rc('text', usetex=False)\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "import pickle\n",
    "import datetime\n",
    "importlib.reload(hetinf_pmd)\n",
    "from math import *\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.sparse\n",
    "from scipy.sparse import dok_matrix, dia_matrix, identity\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change path to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_gpu = 0\n",
    "header = \"djrc_adme_142475\"\n",
    "\n",
    "indicator_serial = 1\n",
    "sub_depth = 5\n",
    "depths = 4\n",
    "steps = 1\n",
    "train_test_split_time_network = \"2016-07-01\"\n",
    "Nodes = hetinf_pmd.Nodes(\"\")\n",
    "Nodes.ParseHeader()\n",
    "Nodes.ClearExtractFiles()\n",
    "node_file = \"core_nodes_\" + header + \".csv\"\n",
    "edge_file = \"extract_subnetwork_\" + header + \"_\" +  str(sub_depth) + \\\n",
    "            \"_version3.csv.gz\"\n",
    "cereal_name = \"/home/rh/Arbeitsraum/Files/KG/All/serial/pmd_\"  + \\\n",
    "             header + \"_\" + train_test_split_time_network + \"_\" + \\\n",
    "             str(depths) + \"step_version3.cereal\"\n",
    "Edges = hetinf_pmd.DeSerialize(cereal_name)\n",
    "print(str(Edges.edge_id_counter) + \",\" + str(Edges.collapsed_edge_id_counter))\n",
    "\n",
    "# FINAL VERSION #    \n",
    "if header == \"djrc_adme_142475\":\n",
    "    # 142475 Nodes found in at least djrc\n",
    "    v1 = [\"Glob_nodes.txt.gz\" for _ in range(4)]\n",
    "    v1.append(\"DJEntity_nodes_1.txt.gz\")\n",
    "    v1.append(\"DJEntity_nodes_1.txt.gz\")\n",
    "    v1.append(\"DJEntity_nodes_1.txt.gz\")\n",
    "    v1.append(\"DJEntity_nodes_1.txt.gz\")\n",
    "    v2 = [\"djrc_sourdate\",\"djrc_adme\",\"djrc_adme2\",\n",
    "          \"djrc_name\",\n",
    "          \"djrc_sourdate\",\"djrc_adme\",\"djrc_adme2\",\n",
    "          \"djrc_name\"]\n",
    "    Nodes = hetinf_pmd.Nodes(\"\")\n",
    "    Nodes.ParseHeader()\n",
    "    Nodes.ClearExtractFiles()\n",
    "    for i in range(len(v1)):\n",
    "        Nodes.extract_files.append(v1[i])\n",
    "        Nodes.extract_variables.append(v2[i])\n",
    "    Nodes.ParseFile()\n",
    "    Nodes.TrimNodeVariables(\"\")\n",
    "core_nodes = list(Nodes.node_variables2.keys())\n",
    "core_nodes = sorted(core_nodes)\n",
    "print(len(core_nodes))\n",
    "\n",
    "node_serial_indicator = 0\n",
    "if 1 == 1:\n",
    "    if node_serial_indicator == 1:\n",
    "        Nodes.ClearCoreNodes()\n",
    "        for i in range(len(core_nodes)):\n",
    "            Nodes.core_nodes.append(core_nodes[i])\n",
    "        Nodes.CreateLabelDataFrame()\n",
    "        dfN = pd.DataFrame({\"node\":Nodes.node_vec,\n",
    "                \"name\":Nodes.name_vec,\"label1\":Nodes.label1_vec,\n",
    "                \"label2\":Nodes.label2_vec,\"start\":Nodes.start_vec})\n",
    "        dfN.to_csv(node_file,index=False,columns=[\"node\",\"name\",\"label1\",\"label2\",\"start\"])\n",
    "    else:\n",
    "        dfN0 = pd.read_csv(node_file,dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADDED: THIS IS USED TO VARY THE STARTING DATE #\n",
    "start_time = \"2010-12-31\"\n",
    "#######################\n",
    "\n",
    "inner_iteration = 100\n",
    "initialize = 1\n",
    "lambda_reg = 0\n",
    "learning_rate = 0.1\n",
    "total_iteration = 10000\n",
    "reduce_dimension = 30\n",
    "threshold_ijyou = 1\n",
    "pattern_activation = 3\n",
    "pattern_loss = 1\n",
    "normalize = 0\n",
    "zero_one = 1\n",
    "max_depth = 4\n",
    "use_label0 = \"Anti-Competitive\"\n",
    "use_label = re.sub(\"/\",\"-\",use_label0)\n",
    "label_serial_indicator = 1\n",
    "mu = 1\n",
    "epsilon = 0\n",
    "cut = 10000\n",
    "bound_left = 0\n",
    "bound_right = 1\n",
    "temp_date0 = datetime.datetime.strptime(\"2017-02-01\", '%Y-%m-%d')\n",
    "if header == \"djrc_adme_142475\":\n",
    "    file_dataframe = \"/path-to-file/core_nodes_djrc_adme_142475.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PrintNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lag_days = 182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates label objects\n",
    "def PrintNum(use_label0,threshold_ijyou,total_iteration,temp_date0,\n",
    "    normalize,pattern_activation,reduce_dimension,initialize,cut,zero_one,\n",
    "    lag_days,feature_dim,use_raw):\n",
    "\n",
    "    # START COMMON #\n",
    "    use_label = re.sub(\"/\",\"-\",use_label0)\n",
    "    Edges.InitializeSparseCoreMatrix(1)\n",
    "    temp_date1 = temp_date0 - datetime.timedelta(days=lag_days)\n",
    "    temp_date2 = temp_date0 + datetime.timedelta(days=50000)\n",
    "    train_test_split_time = datetime.datetime.strftime(temp_date0,\"%Y-%m-%d\")\n",
    "    train_deve_split_time = datetime.datetime.strftime(temp_date1,\"%Y-%m-%d\")\n",
    "    test_end_split_time = datetime.datetime.strftime(temp_date2,\"%Y-%m-%d\")\n",
    "    print(\"train test:\" + str(train_test_split_time) + \",train deve:\" + str(train_deve_split_time))\n",
    "        \n",
    "    # TrainTestをわける\n",
    "    label_serial_indicator = 1\n",
    "    use_min = 0\n",
    "    if label_serial_indicator == 1:\n",
    "        Edges.CreateObjects(file_dataframe,use_label0,\n",
    "                train_test_split_time,test_end_split_time,start_time,use_min)\n",
    "        # global id\n",
    "        train_positive0 = np.asarray(Edges.train_positive_list)\n",
    "        train_positive0 = train_positive0.astype(\"int64\")\n",
    "        test_positive0 = np.asarray(Edges.test_positive_list)\n",
    "        test_positive0 = test_positive0.astype(\"int64\")\n",
    "        objects = []\n",
    "        objects.append(train_positive0)\n",
    "        objects.append(list(Edges.train_positive_time_list))\n",
    "        objects.append(test_positive0)\n",
    "        objects.append(list(Edges.test_positive_time_list))\n",
    "        file_object = header + \"_\" + use_label + \".pkl\"\n",
    "        with open(file_object, mode='wb') as f:\n",
    "            pickle.dump(objects, f)\n",
    "        train_positive_time_list = objects[1]\n",
    "        test_positive_time_list = objects[3]\n",
    "    else:\n",
    "        file_object = header + \"_\" + use_label + \".pkl\"\n",
    "        with open(file_object, mode='rb') as f:\n",
    "            objects = pickle.load(f)\n",
    "        train_positive0 = objects[0]\n",
    "        train_positive_time_list = objects[1]\n",
    "        test_positive0  = objects[2]\n",
    "        test_positive_time_list = objects[3]\n",
    "    #dfD = pd.DataFrame({\"Date\":train_positive_time_list})\n",
    "    #dfD2 = dfD[\"Date\"].value_counts()\n",
    "    #dfD2 = dfD2.reset_index()\n",
    "    #dfD2.columns = [\"Date\",\"Count\"]\n",
    "    #dfTr = dfD2.sort_values(by=\"Date\",ascending=False)\n",
    "    #dfD = pd.DataFrame({\"Date\":test_positive_time_list})\n",
    "    #dfD2 = dfD[\"Date\"].value_counts()\n",
    "    #dfD2 = dfD2.reset_index()\n",
    "    #dfD2.columns = [\"Date\",\"Count\"]\n",
    "    #dfTe = dfD2.sort_values(by=\"Date\",ascending=False)\n",
    "    #dfTr[\"Date\"] = pd.to_datetime(dfTr[\"Date\"])\n",
    "    #plt.plot(dfTr[\"Date\"],dfTr[\"Count\"])\n",
    "    #plt.show()\n",
    "    #dfTe[\"Date\"] = pd.to_datetime(dfTe[\"Date\"])\n",
    "    #plt.plot(dfTe[\"Date\"],dfTe[\"Count\"])\n",
    "    #plt.show()\n",
    "    \n",
    "    print(use_label + \" Num Train: \" + str(len(train_positive0)) + \" Num Test: \" + str(len(test_positive0)))\n",
    "    Edges.ClearTrainPositiveTime()\n",
    "    for i in range(len(train_positive_time_list)):\n",
    "        Edges.train_positive_time.append(train_positive_time_list[i])\n",
    "    # END: COMMON PART #\n",
    "    \n",
    "    ## KOKO\n",
    "    Edges.CalculateCommon(train_positive0,train_deve_split_time,\n",
    "        test_positive0,threshold_ijyou,max_depth,\n",
    "        reduce_dimension,mu,epsilon,10000,zero_one)\n",
    "    print(str(len(Edges.label_prop_id2id)) + \",\" + str(Edges.label_prop_counter))\n",
    "    \n",
    "    label_prop_row_col = []\n",
    "    for i in range(len(Edges.label_prop_row)):\n",
    "        label_prop_row_col.append([Edges.label_prop_row[i],Edges.label_prop_col[i]])\n",
    "        #edge_feature0[i,:] = Edges.label_prop_weight[i,:]\n",
    "\n",
    "    \n",
    "    # ADDED\n",
    "    temp = sorted(label_prop_row_col,key=lambda l:l[1])\n",
    "    label_prop_row_col = sorted(temp,key=lambda l:l[0])\n",
    "    label_prop_row_col = np.asarray(label_prop_row_col,np.int64)\n",
    "    \n",
    "    # This is just for sanity check #\n",
    "    # y_train,y_test,y_full\n",
    "    y_init_train = Edges.y_init_train\n",
    "    y_init_train = np.reshape(y_init_train,[-1,1])\n",
    "    y_init_train = y_init_train.astype(\"float32\")\n",
    "    y_init_test = Edges.y_init_test\n",
    "    y_init_test = np.reshape(y_init_test,[-1,1])\n",
    "    y_init_test = y_init_test.astype(\"float32\")\n",
    "    y_full = Edges.y_full\n",
    "    y_full = np.reshape(y_full,[-1,1])\n",
    "    y_full = y_full.astype(\"float32\")\n",
    "    print(str(np.sum(y_init_train)) + \",\" + \\\n",
    "          str(np.sum(y_init_test)) + \",\" + str(np.sum(y_full)) )\n",
    "    eval_indices_train,eval_indices_train_list = ([] for _ in range(2))\n",
    "    for i in range(len(Edges.eval_indices_train)):\n",
    "        eval_indices_train_list.append(np.int64(Edges.eval_indices_train[i]))\n",
    "        eval_indices_train.append([np.int64(Edges.eval_indices_train[i]),0])\n",
    "    eval_indices_train = np.asarray(eval_indices_train,np.int64)\n",
    "    eval_indices_test,eval_indices_test_list = ([] for _ in range(2))\n",
    "    for i in range(len(Edges.eval_indices_test)):\n",
    "        eval_indices_test_list.append(np.int64(Edges.eval_indices_test[i]))\n",
    "        eval_indices_test.append([np.int64(Edges.eval_indices_test[i]),0])\n",
    "    eval_indices_test = np.asarray(eval_indices_test,np.int64)\n",
    "    print(str(len(eval_indices_train_list)) + \",\" + str(len(eval_indices_test_list)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LP-fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CoreFixed(use_label0,threshold_ijyou,total_iteration,temp_date0,\n",
    "              normalize,initialize,lag_days):\n",
    "    ## HERE ##\n",
    "    pattern_activation = 1\n",
    "    \n",
    "    # START COMMON: 20180920 #\n",
    "    use_label = re.sub(\"/\",\"-\",use_label0)\n",
    "    Edges.InitializeSparseCoreMatrix(1)\n",
    "    temp_date1 = temp_date0 - datetime.timedelta(days=lag_days)\n",
    "    temp_date2 = temp_date0 + datetime.timedelta(days=50000)\n",
    "    train_test_split_time = datetime.datetime.strftime(temp_date0,\"%Y-%m-%d\")\n",
    "    train_deve_split_time = datetime.datetime.strftime(temp_date1,\"%Y-%m-%d\")\n",
    "    test_end_split_time = datetime.datetime.strftime(temp_date2,\"%Y-%m-%d\")\n",
    "    print(\"train test:\" + str(train_test_split_time) + \",train deve:\" + str(train_deve_split_time))\n",
    "    file_object = header + \"_\" + use_label + \".pkl\"\n",
    "    with open(file_object, mode='rb') as f:\n",
    "        objects = pickle.load(f)\n",
    "    train_positive0 = objects[0]\n",
    "    train_positive_time_list = objects[1]\n",
    "    test_positive0  = objects[2]\n",
    "    test_positive_time_list = objects[3]\n",
    "    print(use_label + \" Num Train: \" + str(len(train_positive0)) + \" Num Test: \" + str(len(test_positive0)))\n",
    "    Edges.ClearTrainPositiveTime()\n",
    "    for i in range(len(train_positive_time_list)):\n",
    "        Edges.train_positive_time.append(train_positive_time_list[i])\n",
    "    # END COMMON: 20180920 #\n",
    "    \n",
    "    cut = 10000  \n",
    "    Edges.CreateSparseWeightOneHotBackPath2(train_positive0,\n",
    "            train_deve_split_time,test_positive0,threshold_ijyou,\n",
    "            max_depth,reduce_dimension,mu,epsilon,cut,1)\n",
    "    print(str(len(Edges.label_prop_id2id)) + \",\" + str(Edges.label_prop_counter))\n",
    "    \n",
    "    # START: COMMON PART 20180920 #\n",
    "    edge_feature0 = np.zeros(Edges.label_prop_weight.shape)\n",
    "    label_prop_row_col = []\n",
    "    pair2index = {}\n",
    "    for i in range(len(Edges.label_prop_row)):\n",
    "        pair1 = str(Edges.label_prop_row[i]) + \",\" + str(Edges.label_prop_col[i])\n",
    "        pair2index.update({pair1:i})\n",
    "        label_prop_row_col.append([Edges.label_prop_row[i],Edges.label_prop_col[i]])\n",
    "        edge_feature0[i,:] = Edges.label_prop_weight[i,:]\n",
    "    edge_feature0 = edge_feature0.astype(\"float32\")\n",
    "    edge_feature000 = copy.copy(edge_feature0)\n",
    "    edge_feature00 = copy.copy(edge_feature0)\n",
    "    # ADDED label_prop_row_col wo narabikaetakara edge_feature00 mo douyouni\n",
    "    temp = sorted(label_prop_row_col,key=lambda l:l[1])\n",
    "    label_prop_row_col = sorted(temp,key=lambda l:l[0])\n",
    "    label_prop_row_col = np.asarray(label_prop_row_col,np.int64)    \n",
    "    for i in range(len(Edges.label_prop_row)):\n",
    "        pair1 = str(label_prop_row_col[i,0]) + \",\" + str(label_prop_row_col[i,1])\n",
    "        place = pair2index[pair1]\n",
    "        edge_feature00[i,:] = copy.copy(edge_feature000[place,:])\n",
    "    # END COMMON: 20180920 #\n",
    "    \n",
    "    edge_feature0 = copy.copy(edge_feature00)\n",
    "\n",
    "    # START COMMON: 20180920 #\n",
    "    y_init_train = Edges.y_init_train\n",
    "    y_init_train = np.reshape(y_init_train,[-1,1])\n",
    "    y_init_train = y_init_train.astype(\"float32\")\n",
    "    y_init_test = Edges.y_init_test\n",
    "    y_init_test = np.reshape(y_init_test,[-1,1])\n",
    "    y_init_test = y_init_test.astype(\"float32\")\n",
    "    y_full = Edges.y_full\n",
    "    y_full = np.reshape(y_full,[-1,1])\n",
    "    y_full = y_full.astype(\"float32\")\n",
    "    print(str(np.sum(y_init_train)) + \",\" + str(np.sum(y_init_test)) + \",\" + str(np.sum(y_full)) )\n",
    "    #if one_minus_one == 1:\n",
    "    #    y_init_train = 2*y_init_train - 1\n",
    "    #    y_init_test = 2*y_init_test - 1\n",
    "    #    y_full = 2*y_full - 1\n",
    "    eval_indices_train,eval_indices_train_list = ([] for _ in range(2))\n",
    "    for i in range(len(Edges.eval_indices_train)):\n",
    "        eval_indices_train_list.append(np.int64(Edges.eval_indices_train[i]))\n",
    "        eval_indices_train.append([np.int64(Edges.eval_indices_train[i]),0])\n",
    "    eval_indices_train = np.asarray(eval_indices_train,np.int64)\n",
    "    eval_indices_test,eval_indices_test_list = ([] for _ in range(2))\n",
    "    for i in range(len(Edges.eval_indices_test)):\n",
    "        eval_indices_test_list.append(np.int64(Edges.eval_indices_test[i]))\n",
    "        eval_indices_test.append([np.int64(Edges.eval_indices_test[i]),0])\n",
    "    eval_indices_test = np.asarray(eval_indices_test,np.int64)\n",
    "    print(str(len(eval_indices_train_list)) + \",\" + str(len(eval_indices_test_list)))\n",
    "    label_prop_diagonal = []\n",
    "    for i in range(len(y_full)):\n",
    "        label_prop_diagonal.append([np.int64(i),np.int64(i)])\n",
    "    label_prop_diagonal = np.asarray(label_prop_diagonal,np.int64)\n",
    "    label_prop_inverse_train = Edges.label_prop_inverse_train\n",
    "    label_prop_inverse_train = np.reshape(label_prop_inverse_train,-1)\n",
    "    label_prop_inverse_train = label_prop_inverse_train.astype(\"float32\")\n",
    "    label_prop_inverse_test = Edges.label_prop_inverse_test\n",
    "    label_prop_inverse_test = np.reshape(label_prop_inverse_test ,-1)\n",
    "    label_prop_inverse_test = label_prop_inverse_test.astype(\"float32\")\n",
    "    core_matrix_shape = [len(y_full),len(y_full)]\n",
    "    core_matrix_shape = np.array(core_matrix_shape, dtype=np.int64)\n",
    "    num_edges = edge_feature0.shape[0]\n",
    "    num_features = edge_feature0.shape[1]  \n",
    "    # END COMMON: 20180920 #\n",
    " \n",
    "    # parameter #\n",
    "    alpha0 = 0*np.random.randn(edge_feature0.shape[1]).astype(\"float32\")\n",
    "\n",
    "    ## COMPUTATION GRAPH ##\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if no_gpu == 1:\n",
    "        import os\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" \n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python.framework import ops\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    ## EVALUATION\n",
    "    ops.reset_default_graph()\n",
    "    sess =  tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True))\n",
    "    alpha_init = tf.placeholder(shape=alpha0.shape,dtype=tf.float32)\n",
    "    alpha = tf.Variable(alpha_init)\n",
    "    alpha = tf.reshape(alpha,[num_features,-1])\n",
    "    edge_feature_init = tf.placeholder(tf.float32,shape=edge_feature0.shape)\n",
    "    edge_feature = tf.Variable(edge_feature_init,trainable=False)\n",
    "    # Create label_prop_matrix\n",
    "    if pattern_activation == 1:# Pattern A\n",
    "        edge_weight_data = 2*tf.sigmoid(tf.matmul(edge_feature,alpha))\n",
    "    else:# Pattern B\n",
    "        edge_weight_data = tf.nn.relu(tf.matmul(edge_feature,alpha))\n",
    "    edge_weight_data = tf.reshape(edge_weight_data,[-1])\n",
    "    label_prop_matrix = tf.SparseTensor(indices=label_prop_row_col,values=edge_weight_data,\n",
    "                            dense_shape=tf.constant(core_matrix_shape,tf.int64))\n",
    "    if normalize==1:\n",
    "        b = tf.sparse_reduce_sum(label_prop_matrix,axis=1)\n",
    "        label_prop_matrix2 = label_prop_matrix / tf.reshape(b,(-1, 1))\n",
    "    # Inverse A : in line with MITPress-SemiSupervised Learning Label Propagation\n",
    "    A_inv = tf.SparseTensor(indices=label_prop_diagonal,values=label_prop_inverse_test,\n",
    "                                dense_shape=tf.constant(core_matrix_shape,tf.int64))\n",
    "    if initialize == 1:\n",
    "        f = 0*np.random.uniform(bound_left,bound_right,len(y_init_train))\n",
    "    else:\n",
    "        f = np.random.uniform(bound_left,bound_right,len(y_init_train))\n",
    "    f = f.astype(\"float32\")\n",
    "    f = tf.reshape(f,[len(y_init_test),-1])\n",
    "    for i in range(inner_iteration):\n",
    "        f0 = f\n",
    "        if normalize == 1:\n",
    "            tempB = y_init_test + mu * tf.sparse_tensor_dense_matmul(label_prop_matrix2,f0)\n",
    "        else:\n",
    "            tempB = y_init_test + mu * tf.sparse_tensor_dense_matmul(label_prop_matrix,f0)\n",
    "        tempB = tf.reshape(tempB,(len(y_full),-1)) \n",
    "        f = tf.sparse_tensor_dense_matmul(A_inv,tempB,adjoint_a=False,adjoint_b=False,name=None)\n",
    "    # Retreive test predict\n",
    "    fpre = tf.gather_nd(f,eval_indices_test)\n",
    "    # Regularize\n",
    "    penalty = lambda_reg * tf.reduce_sum(tf.abs(alpha))/float(num_features)\n",
    "    # loss\n",
    "    if pattern_loss == 1:\n",
    "        loss = tf.losses.mean_squared_error(fpre,y_full[eval_indices_test_list,0]) + penalty\n",
    "    else:\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_full[eval_indices_test_list,0],\n",
    "                    logits=(-fpre))/float(len(eval_indices_test_list)) + penalty\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init, feed_dict={alpha_init : 0*alpha0, edge_feature_init : edge_feature0})\n",
    "    for i in range(1):\n",
    "        ofpre,oloss,oalpha = sess.run([fpre,loss,alpha],feed_dict=None)    \n",
    "\n",
    "    # EVALUATE #\n",
    "    ypre = ofpre\n",
    "    ylab= y_full[eval_indices_test_list,0]\n",
    "    # Calculate\n",
    "    random_guess = np.sum(ylab)/len(ylab)\n",
    "    precision, recall, thresholds = precision_recall_curve(ylab,ypre)\n",
    "    area = auc(recall, precision)\n",
    "    print(\"Random Prediction: \" + str(random_guess) + \\\n",
    "            \" AP: \" + str(average_precision_score(ylab, ypre, average='weighted')) + \\\n",
    "            \" ROC: \" + str(roc_auc_score(ylab, ypre)))\n",
    "\n",
    "    # Plot Result\n",
    "    file_pdf = \"figure_20170201/\" + train_test_split_time + \"_corefixed_\" + \\\n",
    "            use_label + \".pdf\"\n",
    "    params = {\n",
    "       'axes.labelsize': 14,\n",
    "       'font.size': 14,\n",
    "       'legend.fontsize': 14,\n",
    "       'xtick.labelsize': 14,\n",
    "       'ytick.labelsize': 14,\n",
    "       'text.usetex': False,\n",
    "       'figure.figsize': [6, 4]\n",
    "    }\n",
    "    plt.rcParams.update(params)\n",
    "    plt.xlabel('Score')\n",
    "    plt.ylabel('True Label')\n",
    "    yint = range(0, 2)\n",
    "    plt.yticks(yint)\n",
    "    plt.plot(ypre,ylab,marker=\"+\",markersize=10,\n",
    "             markeredgewidth=1.5,linewidth=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_pdf)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LP-core-relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneEdge(use_label0,threshold_ijyou,total_iteration,temp_date0,\n",
    "            normalize,pattern_activation,reduce_dimension,initialize,cut,\n",
    "            zero_one,lag_days,one_minus_one,correct_norm,keisu,verbose):\n",
    "    ## HERE ##\n",
    "    max_depth = 1\n",
    "    \n",
    "    # START COMMON: 20180920 #\n",
    "    use_label = re.sub(\"/\",\"-\",use_label0)\n",
    "    Edges.InitializeSparseCoreMatrix(1)\n",
    "    temp_date1 = temp_date0 - datetime.timedelta(days=lag_days)\n",
    "    temp_date2 = temp_date0 + datetime.timedelta(days=50000)\n",
    "    train_test_split_time = datetime.datetime.strftime(temp_date0,\"%Y-%m-%d\")\n",
    "    train_deve_split_time = datetime.datetime.strftime(temp_date1,\"%Y-%m-%d\")\n",
    "    test_end_split_time = datetime.datetime.strftime(temp_date2,\"%Y-%m-%d\")\n",
    "    print(\"train test:\" + str(train_test_split_time) + \",train deve:\" + str(train_deve_split_time))\n",
    "    file_object = header + \"_\" + use_label + \".pkl\"\n",
    "    with open(file_object, mode='rb') as f:\n",
    "        objects = pickle.load(f)\n",
    "    train_positive0 = objects[0]\n",
    "    train_positive_time_list = objects[1]\n",
    "    test_positive0  = objects[2]\n",
    "    test_positive_time_list = objects[3]\n",
    "    print(use_label + \" Num Train: \" + str(len(train_positive0)) + \" Num Test: \" + str(len(test_positive0)))\n",
    "    Edges.ClearTrainPositiveTime()\n",
    "    for i in range(len(train_positive_time_list)):\n",
    "        Edges.train_positive_time.append(train_positive_time_list[i])\n",
    "    # END COMMON: 20180920 #\n",
    "    \n",
    "    if 1 == 0:# Sanity Check OK\n",
    "        dfD = pd.DataFrame({\"Date\":train_positive_time_list})\n",
    "        dfD2 = dfD[\"Date\"].value_counts()\n",
    "        dfD2 = dfD2.reset_index()\n",
    "        dfD2.columns = [\"Date\",\"Count\"]\n",
    "        dfTr = dfD2.sort_values(by=\"Date\",ascending=False)\n",
    "        dfTr[\"Date\"] = pd.to_datetime(dfTr[\"Date\"])\n",
    "        plt.plot(dfTr[\"Date\"],dfTr[\"Count\"])\n",
    "        plt.show()\n",
    "        dfD = pd.DataFrame({\"Date\":test_positive_time_list})\n",
    "        dfD2 = dfD[\"Date\"].value_counts()\n",
    "        dfD2 = dfD2.reset_index()\n",
    "        dfD2.columns = [\"Date\",\"Count\"]\n",
    "        dfTe = dfD2.sort_values(by=\"Date\",ascending=False)\n",
    "        dfTe[\"Date\"] = pd.to_datetime(dfTe[\"Date\"])\n",
    "        plt.plot(dfTe[\"Date\"],dfTe[\"Count\"])\n",
    "        plt.show()\n",
    "        \n",
    "    ## Create Feature\n",
    "    Edges.CreateSparseWeightOneHotBackPath2(train_positive0,\n",
    "        train_deve_split_time,test_positive0,threshold_ijyou,\n",
    "        max_depth,reduce_dimension,mu,epsilon,10000,zero_one)\n",
    "    print(str(len(Edges.label_prop_id2id)) + \",\" + str(Edges.label_prop_counter))\n",
    "    \n",
    "    \n",
    "    # START: COMMON PART 20180920#\n",
    "    edge_feature0 = np.zeros(Edges.label_prop_weight.shape)\n",
    "    label_prop_row_col = []\n",
    "    pair2index = {}\n",
    "    for i in range(len(Edges.label_prop_row)):\n",
    "        pair1 = str(Edges.label_prop_row[i]) + \",\" + str(Edges.label_prop_col[i])\n",
    "        pair2index.update({pair1:i})\n",
    "        label_prop_row_col.append([Edges.label_prop_row[i],Edges.label_prop_col[i]])\n",
    "        edge_feature0[i,:] = Edges.label_prop_weight[i,:]\n",
    "    edge_feature0 = edge_feature0.astype(\"float32\")\n",
    "    edge_feature000 = copy.copy(edge_feature0)\n",
    "    edge_feature00 = copy.copy(edge_feature0)\n",
    "    # ADDED label_prop_row_col wo narabikaetakara edge_feature00 mo douyouni\n",
    "    temp = sorted(label_prop_row_col,key=lambda l:l[1])\n",
    "    label_prop_row_col = sorted(temp,key=lambda l:l[0])\n",
    "    label_prop_row_col = np.asarray(label_prop_row_col,np.int64)    \n",
    "    for i in range(len(Edges.label_prop_row)):\n",
    "        pair1 = str(label_prop_row_col[i,0]) + \",\" + str(label_prop_row_col[i,1])\n",
    "        place = pair2index[pair1]\n",
    "        edge_feature00[i,:] = copy.copy(edge_feature000[place,:])\n",
    "    # END COMMON: 20180920 #\n",
    " \n",
    "    # Sanity Check\n",
    "    if 1==0:\n",
    "        for i in [133,23421,4]:\n",
    "            row0 = label_prop_row_col[i,0]\n",
    "            col0 = label_prop_row_col[i,1]\n",
    "            pair0 = str(row0) + \",\" + str(col0)\n",
    "            print(all(edge_feature00[i,:]==edge_feature0[pair2index[pair0],:]))\n",
    "        i = 1020\n",
    "        dfE0 = pd.DataFrame(label_prop_row_col)\n",
    "        dfE0.columns = [\"source\",\"target\"]\n",
    "        source = dfE0[\"source\"].iloc[i]\n",
    "        target = dfE0[\"target\"].iloc[i]\n",
    "        cond = (dfE0[\"source\"] == source) & (dfE0[\"target\"] == target)\n",
    "        index_a = dfE0.loc[cond].index[0]\n",
    "        cond = (dfE0[\"source\"] == target) & (dfE0[\"target\"] == source)\n",
    "        index_b = dfE0.loc[cond].index[0]\n",
    "        print(all(edge_feature00[index_a,:] == edge_feature00[index_b,:]))\n",
    "     \n",
    "    \n",
    "    # START COMMON: 20180920 #\n",
    "    y_init_train = Edges.y_init_train\n",
    "    y_init_train = np.reshape(y_init_train,[-1,1])\n",
    "    y_init_train = y_init_train.astype(\"float32\")\n",
    "    y_init_test = Edges.y_init_test\n",
    "    y_init_test = np.reshape(y_init_test,[-1,1])\n",
    "    y_init_test = y_init_test.astype(\"float32\")\n",
    "    y_full = Edges.y_full\n",
    "    y_full = np.reshape(y_full,[-1,1])\n",
    "    y_full = y_full.astype(\"float32\")\n",
    "    print(str(np.sum(y_init_train)) + \",\" + str(np.sum(y_init_test)) + \",\" + str(np.sum(y_full)) )\n",
    "    #if one_minus_one == 1:\n",
    "    #    y_init_train = 2*y_init_train - 1\n",
    "    #    y_init_test = 2*y_init_test - 1\n",
    "    #    y_full = 2*y_full - 1\n",
    "    eval_indices_train,eval_indices_train_list = ([] for _ in range(2))\n",
    "    for i in range(len(Edges.eval_indices_train)):\n",
    "        eval_indices_train_list.append(np.int64(Edges.eval_indices_train[i]))\n",
    "        eval_indices_train.append([np.int64(Edges.eval_indices_train[i]),0])\n",
    "    eval_indices_train = np.asarray(eval_indices_train,np.int64)\n",
    "    eval_indices_test,eval_indices_test_list = ([] for _ in range(2))\n",
    "    for i in range(len(Edges.eval_indices_test)):\n",
    "        eval_indices_test_list.append(np.int64(Edges.eval_indices_test[i]))\n",
    "        eval_indices_test.append([np.int64(Edges.eval_indices_test[i]),0])\n",
    "    eval_indices_test = np.asarray(eval_indices_test,np.int64)\n",
    "    print(str(len(eval_indices_train_list)) + \",\" + str(len(eval_indices_test_list)))\n",
    "    label_prop_diagonal = []\n",
    "    for i in range(len(y_full)):\n",
    "        label_prop_diagonal.append([np.int64(i),np.int64(i)])\n",
    "    label_prop_diagonal = np.asarray(label_prop_diagonal,np.int64)\n",
    "    label_prop_inverse_train = Edges.label_prop_inverse_train\n",
    "    label_prop_inverse_train = np.reshape(label_prop_inverse_train,-1)\n",
    "    label_prop_inverse_train = label_prop_inverse_train.astype(\"float32\")\n",
    "    label_prop_inverse_test = Edges.label_prop_inverse_test\n",
    "    label_prop_inverse_test = np.reshape(label_prop_inverse_test ,-1)\n",
    "    label_prop_inverse_test = label_prop_inverse_test.astype(\"float32\")\n",
    "    core_matrix_shape = [len(y_full),len(y_full)]\n",
    "    core_matrix_shape = np.array(core_matrix_shape, dtype=np.int64)\n",
    "    num_edges = edge_feature0.shape[0]\n",
    "    num_features = edge_feature0.shape[1]  \n",
    "    # END COMMON: 20180920 #\n",
    "\n",
    "    label_prop_I_train = Edges.label_prop_I_train\n",
    "    label_prop_I_train = np.reshape(label_prop_I_train,-1)\n",
    "    label_prop_I_train = label_prop_I_train.astype(\"float32\")\n",
    "    label_prop_I_test = Edges.label_prop_I_test\n",
    "    label_prop_I_test = np.reshape(label_prop_I_test,-1)\n",
    "    label_prop_I_test = label_prop_I_test.astype(\"float32\")\n",
    "    num_use = edge_feature00.shape[1]\n",
    "    trim_id2id = {}\n",
    "    cnt = 0\n",
    "    for i in range(num_use):\n",
    "        if np.sum(edge_feature00[:,i]) > 0:\n",
    "            trim_id2id.update({cnt:i})\n",
    "            cnt += 1\n",
    "    edge_feature1 = np.zeros([edge_feature00.shape[0],cnt])\n",
    "    for i in range(cnt):\n",
    "        edge_feature1[:,i] = edge_feature00[:,trim_id2id[i]]\n",
    "    edge_feature0 = copy.copy(edge_feature1)\n",
    "\n",
    "    # TRAIN parameter\n",
    "    if (pattern_activation == 3) | (pattern_activation == 4):\n",
    "        alpha10 = keisu*np.random.randn(edge_feature0.shape[1],reduce_dimension).astype(\"float32\")\n",
    "        beta10  = keisu*np.random.randn(reduce_dimension).astype(\"float32\")\n",
    "        alpha20 = keisu*np.random.randn(reduce_dimension).astype(\"float32\")\n",
    "        beta20 =  keisu*np.random.randn(1).astype(\"float32\")\n",
    "    elif pattern_activation == 2:\n",
    "        alpha0 = keisu*np.random.randn(edge_feature0.shape[1]).astype(\"float32\")\n",
    "        beta0 = keisu*np.random.randn(1).astype(\"float32\")\n",
    "    else:\n",
    "        alpha0 = np.random.randn(edge_feature0.shape[1]).astype(\"float32\")\n",
    "    \n",
    "    ## COMPUTATION GRAPH ##\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if no_gpu == 1:\n",
    "        import os\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" \n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python.framework import ops\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    ops.reset_default_graph()\n",
    "    sess =  tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True))\n",
    "    if (pattern_activation == 3) | (pattern_activation == 4):\n",
    "        alpha1_init = tf.placeholder(shape=alpha10.shape,dtype=tf.float32)\n",
    "        alpha1 = tf.Variable(alpha1_init)\n",
    "        alpha1 = tf.reshape(alpha1,[edge_feature0.shape[1],-1])\n",
    "        beta1_init = tf.placeholder(shape=beta10.shape,dtype=tf.float32)\n",
    "        beta1 = tf.Variable(beta1_init)\n",
    "        beta1 = tf.reshape(beta1,[-1,reduce_dimension])\n",
    "        alpha2_init = tf.placeholder(shape=alpha20.shape,dtype=tf.float32)\n",
    "        alpha2 = tf.Variable(alpha2_init)\n",
    "        alpha2 = tf.reshape(alpha2,[reduce_dimension,-1])\n",
    "        beta2_init = tf.placeholder(shape=beta20.shape,dtype=tf.float32)\n",
    "        beta2 = tf.Variable(beta2_init)\n",
    "    elif (pattern_activation == 2):\n",
    "        alpha_init = tf.placeholder(shape=alpha0.shape,dtype=tf.float32)\n",
    "        alpha = tf.Variable(alpha_init)\n",
    "        alpha = tf.reshape(alpha,[edge_feature0.shape[1],-1])\n",
    "        beta_init = tf.placeholder(shape=beta0.shape,dtype=tf.float32)\n",
    "        beta = tf.Variable(beta_init)\n",
    "    else:\n",
    "        alpha_init = tf.placeholder(shape=alpha0.shape,dtype=tf.float32)\n",
    "        alpha = tf.Variable(alpha_init)    \n",
    "        alpha = tf.reshape(alpha,[num_features,-1])\n",
    "    edge_feature_init = tf.placeholder(tf.float32, shape=edge_feature0.shape)\n",
    "    edge_feature = tf.Variable(edge_feature_init,trainable=False)\n",
    "\n",
    "    # Create label_prop_matrix\n",
    "    if pattern_activation == 3:# Pattern C\n",
    "        middle = tf.sigmoid(tf.matmul(edge_feature,alpha1) + beta1)\n",
    "        edge_weight_data = tf.sigmoid(tf.matmul(middle,alpha2) + beta2)\n",
    "    elif pattern_activation == 4:# Pattern D\n",
    "        middle = tf.nn.relu(tf.matmul(edge_feature,alpha1) + beta1)\n",
    "        edge_weight_data = tf.sigmoid(tf.matmul(middle,alpha2) + beta2) \n",
    "    elif pattern_activation == 1:# Pattern A\n",
    "        edge_weight_data = tf.sigmoid(tf.matmul(edge_feature,alpha))\n",
    "    elif pattern_activation == 2:# Pattern B\n",
    "        edge_weight_data = tf.sigmoid(tf.matmul(edge_feature,alpha) + beta)\n",
    "    edge_weight_data = tf.reshape(edge_weight_data,[-1])\n",
    "    label_prop_matrix = tf.SparseTensor(indices=label_prop_row_col,values=edge_weight_data,\n",
    "                        dense_shape=tf.constant(core_matrix_shape,tf.int64))    \n",
    "    if correct_norm == 1:\n",
    "        label_prop_matrix_rowsum0 = tf.sparse_reduce_sum(label_prop_matrix, 1)\n",
    "        label_prop_matrix_rowsum1 = label_prop_matrix_rowsum0 + label_prop_I_train\n",
    "        label_prop_matrix_rowsum_inverse = 1.0/label_prop_matrix_rowsum1\n",
    "    if normalize==1:\n",
    "        b = tf.sparse_reduce_sum(label_prop_matrix, axis=1)\n",
    "        label_prop_matrix2 = label_prop_matrix / tf.reshape(b, (-1, 1))\n",
    "    # Inverse A :\n",
    "    if correct_norm == 1:\n",
    "        A_inv = tf.SparseTensor(indices=label_prop_diagonal,\n",
    "                values=label_prop_matrix_rowsum_inverse,dense_shape=tf.constant(core_matrix_shape,tf.int64))     \n",
    "    else:\n",
    "        A_inv = tf.SparseTensor(indices=label_prop_diagonal,\n",
    "                values=label_prop_inverse_train,dense_shape=tf.constant(core_matrix_shape,tf.int64))\n",
    "    if initialize == 1:\n",
    "        if one_minus_one == 1:\n",
    "            f = 0*np.random.uniform(bound_left,bound_right,len(y_init_train)) - 1\n",
    "        else:\n",
    "            f = 0*np.random.uniform(bound_left,bound_right,len(y_init_train))\n",
    "    else:\n",
    "        f = np.random.uniform(bound_left,bound_right,len(y_init_train))\n",
    "    f = f.astype(\"float32\")\n",
    "    f = tf.reshape(f,[len(y_init_train),-1])\n",
    "    for i in range(inner_iteration):\n",
    "        f0 = f\n",
    "        if normalize == 1:\n",
    "            tempB = y_init_train + mu * tf.sparse_tensor_dense_matmul(label_prop_matrix2,f0)\n",
    "        else:\n",
    "            tempB = y_init_train + mu * tf.sparse_tensor_dense_matmul(label_prop_matrix,f0)\n",
    "        tempB = tf.reshape(tempB,(len(y_full),-1)) \n",
    "        f = tf.sparse_tensor_dense_matmul(A_inv,tempB,adjoint_a=False,adjoint_b=False,name=None)\n",
    "    \n",
    "    # Retreive test predict\n",
    "    fpre = tf.gather_nd(f,eval_indices_train)\n",
    "    # Regularize\n",
    "    if (pattern_activation == 3) | (pattern_activation == 4):\n",
    "        penalty = lambda_reg*tf.nn.l2_loss(alpha1) + lambda_reg*tf.nn.l2_loss(alpha2)\n",
    "    else:\n",
    "        penalty = lambda_reg*tf.nn.l2_loss(alpha)\n",
    "    # loss\n",
    "    if pattern_loss == 1:\n",
    "        loss = tf.losses.mean_squared_error(fpre,y_full[eval_indices_train_list,0]) + penalty\n",
    "    else:\n",
    "        fpre2 = fpre + tf.constant(0.001)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_full[eval_indices_train_list,0],\n",
    "                    logits=(tf.log(fpre2)))/float(len(eval_indices_train_list)) + penalty\n",
    "    \n",
    "    # Optimizer\n",
    "    my_opt = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "    train_step = my_opt.minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    if (pattern_activation == 3) | (pattern_activation==4):\n",
    "        sess.run(init,feed_dict={alpha1_init:alpha10,beta1_init:beta10,alpha2_init:alpha20,\n",
    "                                 beta2_init:beta20,edge_feature_init:edge_feature0})\n",
    "    elif pattern_activation == 2:    \n",
    "        sess.run(init,feed_dict={alpha_init:alpha0,beta_init:beta0,edge_feature_init:edge_feature0})\n",
    "    else:\n",
    "        sess.run(init,feed_dict={alpha_init:alpha0,edge_feature_init:edge_feature0})\n",
    "  \n",
    "    #otest1,otest2,otest3= sess.run([label_prop_matrix_rowsum0,\n",
    "    #                               label_prop_matrix_rowsum1,\n",
    "    #                               label_prop_matrix_rowsum_inverse])\n",
    "    \n",
    "    # Clear loss tracker\n",
    "    track_loss = []\n",
    "    for itr in range(total_iteration):\n",
    "        _ = sess.run([train_step],feed_dict=None)\n",
    "        if (pattern_activation == 3) | (pattern_activation == 4):\n",
    "            oloss,oalpha1,obeta1,oalpha2,obeta2,ofpre = sess.run([loss,\n",
    "                                        alpha1,beta1,alpha2,beta2,fpre])\n",
    "        elif(pattern_activation == 2):\n",
    "            oloss,oalpha,obeta,ofpre = sess.run([loss,alpha,beta,fpre],feed_dict=None)  \n",
    "        else:\n",
    "            oloss,oalpha,ofpre = sess.run([loss,alpha,fpre],feed_dict=None)   \n",
    "        if (itr % 500 == 0) | (itr == 0):\n",
    "            track_loss.append(oloss)\n",
    "            if verbose == 1:\n",
    "                print(\"Iteration: \" + str(itr) + \" Loss: \" + str(oloss))\n",
    "        if (itr % total_iteration == 0) | (itr == 0):\n",
    "            print(\"Iteration: \" + str(itr) + \" Loss: \" + str(oloss))    \n",
    "    final_train_loss = str(10000000000*oloss)\n",
    "    final_train_loss = final_train_loss[:5]\n",
    "    print(\"Final Loss: \" + str(oloss))\n",
    "\n",
    "    # EVALUATION #\n",
    "    ops.reset_default_graph()\n",
    "    sess =  tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True))\n",
    "    if (pattern_activation == 3) | (pattern_activation == 4):\n",
    "        alpha1_init = tf.placeholder(shape=alpha10.shape,dtype=tf.float32)\n",
    "        alpha1 = tf.Variable(alpha1_init)\n",
    "        alpha1 = tf.reshape(alpha1,[edge_feature0.shape[1],-1])\n",
    "        beta1_init = tf.placeholder(shape=beta10.shape,dtype=tf.float32)\n",
    "        beta1 = tf.Variable(beta1_init)\n",
    "        beta1 = tf.reshape(beta1,[-1,reduce_dimension])\n",
    "        alpha2_init = tf.placeholder(shape=alpha20.shape,dtype=tf.float32)\n",
    "        alpha2 = tf.Variable(alpha2_init)\n",
    "        alpha2 = tf.reshape(alpha2,[reduce_dimension,-1])\n",
    "        beta2_init = tf.placeholder(shape=beta20.shape,dtype=tf.float32)\n",
    "        beta2 = tf.Variable(beta2_init)\n",
    "    elif (pattern_activation == 2):\n",
    "        alpha_init = tf.placeholder(shape=alpha0.shape,dtype=tf.float32)\n",
    "        alpha = tf.Variable(alpha_init)\n",
    "        alpha = tf.reshape(alpha,[edge_feature0.shape[1],-1])\n",
    "        beta_init = tf.placeholder(shape=beta0.shape,dtype=tf.float32)\n",
    "        beta = tf.Variable(beta_init)\n",
    "    else:\n",
    "        alpha_init = tf.placeholder(shape=alpha0.shape,dtype=tf.float32)\n",
    "        alpha = tf.Variable(alpha_init)    \n",
    "        alpha = tf.reshape(alpha,[num_features,-1])\n",
    "    edge_feature_init = tf.placeholder(tf.float32, shape=edge_feature0.shape)\n",
    "    edge_feature = tf.Variable(edge_feature_init,trainable=False)\n",
    "    \n",
    "    # Create label_prop_matrix\n",
    "    if pattern_activation == 3:# Pattern C\n",
    "        middle = tf.sigmoid(tf.matmul(edge_feature,alpha1) + beta1)\n",
    "        edge_weight_data = tf.sigmoid(tf.matmul(middle,alpha2) + beta2)\n",
    "    elif pattern_activation == 4:# Pattern D\n",
    "        middle = tf.nn.relu(tf.matmul(edge_feature,alpha1) + beta1)\n",
    "        edge_weight_data = tf.sigmoid(tf.matmul(middle,alpha2) + beta2) \n",
    "    elif pattern_activation == 1:# Pattern A\n",
    "        edge_weight_data = tf.sigmoid(tf.matmul(edge_feature,alpha))\n",
    "    elif pattern_activation == 2:# Pattern B\n",
    "        edge_weight_data = tf.sigmoid(tf.matmul(edge_feature,alpha) + beta)\n",
    "    edge_weight_data = tf.reshape(edge_weight_data,[-1])\n",
    "    label_prop_matrix = tf.SparseTensor(indices=label_prop_row_col,values=edge_weight_data,\n",
    "                        dense_shape=tf.constant(core_matrix_shape,tf.int64))    \n",
    "    if normalize==1:\n",
    "        b = tf.sparse_reduce_sum(label_prop_matrix, axis=1)\n",
    "        label_prop_matrix2 = label_prop_matrix / tf.reshape(b, (-1, 1))\n",
    "    if correct_norm == 1:\n",
    "        label_prop_matrix_rowsum0 = tf.sparse_reduce_sum(label_prop_matrix, 1)\n",
    "        label_prop_matrix_rowsum1 = label_prop_matrix_rowsum0 + label_prop_I_test\n",
    "        label_prop_matrix_rowsum_inverse = 1.0/label_prop_matrix_rowsum1\n",
    "    \n",
    "    # Inverse A\n",
    "    if correct_norm == 1:\n",
    "        A_inv = tf.SparseTensor(indices=label_prop_diagonal,\n",
    "                    values=label_prop_matrix_rowsum_inverse,\n",
    "                    dense_shape=tf.constant(core_matrix_shape,tf.int64))\n",
    "    else: \n",
    "        A_inv = tf.SparseTensor(indices=label_prop_diagonal,\n",
    "                    values=label_prop_inverse_test,\n",
    "                    dense_shape=tf.constant(core_matrix_shape,tf.int64))\n",
    "    if initialize == 1:\n",
    "        f = 0*np.random.uniform(bound_left,bound_right,len(y_init_test))\n",
    "    else:\n",
    "        f = np.random.uniform(bound_left,bound_right,len(y_init_test))\n",
    "    f = f.astype(\"float32\")\n",
    "    f = tf.reshape(f,[len(y_init_test),-1])\n",
    "    for i in range(inner_iteration):\n",
    "        f0 = f\n",
    "        if normalize == 1:\n",
    "            tempB = y_init_test + mu * tf.sparse_tensor_dense_matmul(label_prop_matrix2,f0)\n",
    "        else:\n",
    "            tempB = y_init_test + mu * tf.sparse_tensor_dense_matmul(label_prop_matrix,f0)\n",
    "        tempB = tf.reshape(tempB,(len(y_full),-1)) \n",
    "        f = tf.sparse_tensor_dense_matmul(A_inv,tempB,adjoint_a=False,adjoint_b=False,name=None)\n",
    "    # Retreive test predict\n",
    "    fpre = tf.gather_nd(f,eval_indices_test)\n",
    "    # Regularize\n",
    "    if (pattern_activation == 3) | (pattern_activation == 4):\n",
    "        penalty = lambda_reg * tf.reduce_sum(tf.abs(alpha1))/float(num_features)\n",
    "    else:\n",
    "        penalty = lambda_reg * tf.reduce_sum(tf.abs(alpha))/float(num_features)\n",
    "    # loss\n",
    "    if pattern_loss == 1:\n",
    "        loss = tf.losses.mean_squared_error(fpre,y_full[eval_indices_test_list,0]) + penalty\n",
    "    else:\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_full[eval_indices_test_list,0],\n",
    "                    logits=(-fpre))/float(len(eval_indices_test_list)) + penalty\n",
    "    # Optimizer\n",
    "    my_opt = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "    train_step = my_opt.minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    if (pattern_activation == 3) | (pattern_activation==4):\n",
    "        sess.run(init,feed_dict={alpha1_init:oalpha1,beta1_init:obeta1[0,:],\n",
    "                alpha2_init:oalpha2[:,0],beta2_init:obeta2,\n",
    "                edge_feature_init:edge_feature0})\n",
    "    elif pattern_activation == 2:    \n",
    "        sess.run(init,feed_dict={alpha_init:oalpha[:,0],beta_init:obeta,\n",
    "                edge_feature_init:edge_feature0})\n",
    "    else:\n",
    "        sess.run(init,feed_dict={alpha_init:oalpha[:,0],edge_feature_init:edge_feature0})\n",
    "    for itr in range(1):\n",
    "        if (pattern_activation == 3) | (pattern_activation == 4):\n",
    "            oloss,oalpha1,obeta1,oalpha2,obeta2,ofpre,out_edge_weight = sess.run([loss,\n",
    "                                        alpha1,beta1,alpha2,beta2,fpre,edge_weight_data])\n",
    "        elif(pattern_activation == 2):\n",
    "            oloss,oalpha,obeta,ofpre,out_edge_weight = sess.run([loss,alpha,beta,fpre,edge_weight_data],feed_dict=None)  \n",
    "        else:\n",
    "            oloss,oalpha,ofpre,out_edge_weight = sess.run([loss,alpha,fpre,edge_weight_data],feed_dict=None)   \n",
    "\n",
    "    # Evaluate\n",
    "    ypre = ofpre\n",
    "    ylab= y_full[eval_indices_test_list]\n",
    "    # Calculate\n",
    "    random_guess = np.sum(ylab)/len(ylab)\n",
    "    precision, recall, thresholds = precision_recall_curve(ylab,ypre)\n",
    "    area = auc(recall, precision)\n",
    "    print(\"Random Prediction: \" + str(random_guess) + \\\n",
    "          \" AP: \" + str(average_precision_score(ylab, ypre, average='weighted')) + \\\n",
    "          \" ROC: \" + str(roc_auc_score(ylab, ypre)))\n",
    "\n",
    "    file_pdf = \"figure_20170201/\" + train_test_split_time + \"_oneedge_\" + \\\n",
    "            use_label + \".pdf\"\n",
    "    params = {\n",
    "       'axes.labelsize': 14,\n",
    "       'font.size': 14,\n",
    "       'legend.fontsize': 14,\n",
    "       'xtick.labelsize': 14,\n",
    "       'ytick.labelsize': 14,\n",
    "       'text.usetex': False,\n",
    "       'figure.figsize': [6, 4]\n",
    "    }\n",
    "    plt.rcParams.update(params)\n",
    "    plt.xlabel('Score')\n",
    "    plt.ylabel('True Label')\n",
    "    yint = range(0, 2)\n",
    "    plt.yticks(yint)\n",
    "    plt.plot(ypre,ylab,marker=\"+\",markersize=10,\n",
    "             markeredgewidth=1.5,linewidth=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_pdf)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LP-path-segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound_left = 0\n",
    "bound_right = 1\n",
    "initialize = 1\n",
    "temp_date0 = datetime.datetime.strptime(\"2017-02-01\",\"%Y-%m-%d\")\n",
    "lag_days = 31\n",
    "feature_dim = 50\n",
    "use_raw = 1\n",
    "one_minus_one = 0\n",
    "correct_norm = 0\n",
    "epsilon = 0.0\n",
    "verbose = 1\n",
    "use_label0 = \"Product/Service\"\n",
    "keisu = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneBP2(use_label0,threshold_ijyou,total_iteration,temp_date0,\n",
    "        normalize,pattern_activation,reduce_dimension,initialize,cut,\n",
    "        zero_one,lag_days,feature_dim,use_raw,one_minus_one,\n",
    "        correct_norm,epsilon,keisu,verbose):\n",
    "    \n",
    "    # START COMMON: 20180920 #\n",
    "    use_label = re.sub(\"/\",\"-\",use_label0)\n",
    "    Edges.InitializeSparseCoreMatrix(1)\n",
    "    temp_date1 = temp_date0 - datetime.timedelta(days=lag_days)\n",
    "    temp_date2 = temp_date0 + datetime.timedelta(days=50000)\n",
    "    train_test_split_time = datetime.datetime.strftime(temp_date0,\"%Y-%m-%d\")\n",
    "    train_deve_split_time = datetime.datetime.strftime(temp_date1,\"%Y-%m-%d\")\n",
    "    test_end_split_time = datetime.datetime.strftime(temp_date2,\"%Y-%m-%d\")\n",
    "    print(\"train test:\" + str(train_test_split_time) + \",train deve:\" + str(train_deve_split_time))\n",
    "    file_object = header + \"_\" + use_label + \".pkl\"\n",
    "    with open(file_object, mode='rb') as f:\n",
    "        objects = pickle.load(f)\n",
    "    train_positive0 = objects[0]\n",
    "    train_positive_time_list = objects[1]\n",
    "    test_positive0  = objects[2]\n",
    "    test_positive_time_list = objects[3]\n",
    "    print(use_label + \" Num Train: \" + str(len(train_positive0)) + \" Num Test: \" + str(len(test_positive0)))\n",
    "    Edges.ClearTrainPositiveTime()\n",
    "    for i in range(len(train_positive_time_list)):\n",
    "        Edges.train_positive_time.append(train_positive_time_list[i])\n",
    "    # END COMMON: 20180920 #\n",
    "    \n",
    "    if 1 == 0:# Sanity Check OK\n",
    "        dfD = pd.DataFrame({\"Date\":train_positive_time_list})\n",
    "        dfD2 = dfD[\"Date\"].value_counts()\n",
    "        dfD2 = dfD2.reset_index()\n",
    "        dfD2.columns = [\"Date\",\"Count\"]\n",
    "        dfTr = dfD2.sort_values(by=\"Date\",ascending=False)\n",
    "        dfTr[\"Date\"] = pd.to_datetime(dfTr[\"Date\"])\n",
    "        plt.plot(dfTr[\"Date\"],dfTr[\"Count\"])\n",
    "        plt.show()\n",
    "        dfD = pd.DataFrame({\"Date\":test_positive_time_list})\n",
    "        dfD2 = dfD[\"Date\"].value_counts()\n",
    "        dfD2 = dfD2.reset_index()\n",
    "        dfD2.columns = [\"Date\",\"Count\"]\n",
    "        dfTe = dfD2.sort_values(by=\"Date\",ascending=False)\n",
    "        dfTe[\"Date\"] = pd.to_datetime(dfTe[\"Date\"])\n",
    "        plt.plot(dfTe[\"Date\"],dfTe[\"Count\"])\n",
    "        plt.show()\n",
    "        \n",
    "    ## Create Feature\n",
    "    Edges.CreateSparseWeightOneHotBackPath2(train_positive0,\n",
    "        train_deve_split_time,test_positive0,threshold_ijyou,\n",
    "        max_depth,reduce_dimension,mu,epsilon,10000,zero_one)\n",
    "    print(str(len(Edges.label_prop_id2id)) + \",\" + str(Edges.label_prop_counter))\n",
    "    # START: COMMON PART 20180920#\n",
    "    edge_feature0 = np.zeros(Edges.label_prop_weight.shape)\n",
    "    label_prop_row_col = []\n",
    "    pair2index = {}\n",
    "    for i in range(len(Edges.label_prop_row)):\n",
    "        pair1 = str(Edges.label_prop_row[i]) + \",\" + str(Edges.label_prop_col[i])\n",
    "        pair2index.update({pair1:i})\n",
    "        label_prop_row_col.append([Edges.label_prop_row[i],Edges.label_prop_col[i]])\n",
    "        edge_feature0[i,:] = Edges.label_prop_weight[i,:]\n",
    "    edge_feature0 = edge_feature0.astype(\"float32\")\n",
    "    edge_feature000 = copy.copy(edge_feature0)\n",
    "    edge_feature00 = copy.copy(edge_feature0)\n",
    "    # ADDED label_prop_row_col wo narabikaetakara edge_feature00 mo douyouni\n",
    "    temp = sorted(label_prop_row_col,key=lambda l:l[1])\n",
    "    label_prop_row_col = sorted(temp,key=lambda l:l[0])\n",
    "    label_prop_row_col = np.asarray(label_prop_row_col,np.int64)    \n",
    "    for i in range(len(Edges.label_prop_row)):\n",
    "        pair1 = str(label_prop_row_col[i,0]) + \",\" + str(label_prop_row_col[i,1])\n",
    "        place = pair2index[pair1]\n",
    "        edge_feature00[i,:] = copy.copy(edge_feature000[place,:])\n",
    "    # END COMMON: 20180920 #\n",
    "    \n",
    "    #Edges.CreateObjectsTrim(file_dataframe)\n",
    "    # Sanity Check\n",
    "    if 1==0:\n",
    "        for i in [133,23421,4]:\n",
    "            row0 = label_prop_row_col[i,0]\n",
    "            col0 = label_prop_row_col[i,1]\n",
    "            pair0 = str(row0) + \",\" + str(col0)\n",
    "            print(all(edge_feature00[i,:]==edge_feature0[pair2index[pair0],:]))\n",
    "        i = 1020\n",
    "        dfE0 = pd.DataFrame(label_prop_row_col)\n",
    "        dfE0.columns = [\"source\",\"target\"]\n",
    "        source = dfE0[\"source\"].iloc[i]\n",
    "        target = dfE0[\"target\"].iloc[i]\n",
    "        cond = (dfE0[\"source\"] == source) & (dfE0[\"target\"] == target)\n",
    "        index_a = dfE0.loc[cond].index[0]\n",
    "        cond = (dfE0[\"source\"] == target) & (dfE0[\"target\"] == source)\n",
    "        index_b = dfE0.loc[cond].index[0]\n",
    "        print(all(edge_feature00[index_a,:] == edge_feature00[index_b,:]))\n",
    "        \n",
    "    # START COMMON: 20180920 #\n",
    "    y_init_train = Edges.y_init_train\n",
    "    y_init_train = np.reshape(y_init_train,[-1,1])\n",
    "    y_init_train = y_init_train.astype(\"float32\")\n",
    "    y_init_test = Edges.y_init_test\n",
    "    y_init_test = np.reshape(y_init_test,[-1,1])\n",
    "    y_init_test = y_init_test.astype(\"float32\")\n",
    "    y_full = Edges.y_full\n",
    "    y_full = np.reshape(y_full,[-1,1])\n",
    "    y_full = y_full.astype(\"float32\")\n",
    "    print(str(np.sum(y_init_train)) + \",\" + str(np.sum(y_init_test)) + \",\" + str(np.sum(y_full)) )\n",
    "    #if one_minus_one == 1:\n",
    "    #    y_init_train = 2*y_init_train - 1\n",
    "    #    y_init_test = 2*y_init_test - 1\n",
    "    #    y_full = 2*y_full - 1\n",
    "    eval_indices_train,eval_indices_train_list = ([] for _ in range(2))\n",
    "    for i in range(len(Edges.eval_indices_train)):\n",
    "        eval_indices_train_list.append(np.int64(Edges.eval_indices_train[i]))\n",
    "        eval_indices_train.append([np.int64(Edges.eval_indices_train[i]),0])\n",
    "    eval_indices_train = np.asarray(eval_indices_train,np.int64)\n",
    "    eval_indices_test,eval_indices_test_list = ([] for _ in range(2))\n",
    "    for i in range(len(Edges.eval_indices_test)):\n",
    "        eval_indices_test_list.append(np.int64(Edges.eval_indices_test[i]))\n",
    "        eval_indices_test.append([np.int64(Edges.eval_indices_test[i]),0])\n",
    "    eval_indices_test = np.asarray(eval_indices_test,np.int64)\n",
    "    print(str(len(eval_indices_train_list)) + \",\" + str(len(eval_indices_test_list)))\n",
    "    label_prop_diagonal = []\n",
    "    for i in range(len(y_full)):\n",
    "        label_prop_diagonal.append([np.int64(i),np.int64(i)])\n",
    "    label_prop_diagonal = np.asarray(label_prop_diagonal,np.int64)\n",
    "    label_prop_inverse_train = Edges.label_prop_inverse_train\n",
    "    label_prop_inverse_train = np.reshape(label_prop_inverse_train,-1)\n",
    "    label_prop_inverse_train = label_prop_inverse_train.astype(\"float32\")\n",
    "    label_prop_inverse_test = Edges.label_prop_inverse_test\n",
    "    label_prop_inverse_test = np.reshape(label_prop_inverse_test ,-1)\n",
    "    label_prop_inverse_test = label_prop_inverse_test.astype(\"float32\")\n",
    "    core_matrix_shape = [len(y_full),len(y_full)]\n",
    "    core_matrix_shape = np.array(core_matrix_shape, dtype=np.int64)\n",
    "    num_edges = edge_feature0.shape[0]\n",
    "    num_features = edge_feature0.shape[1]  \n",
    "    # END COMMON: 20180920 #    \n",
    "    \n",
    "    # NEW!\n",
    "    label_prop_I_train = Edges.label_prop_I_train\n",
    "    label_prop_I_train = np.reshape(label_prop_I_train,-1)\n",
    "    label_prop_I_train = label_prop_I_train.astype(\"float32\")\n",
    "    label_prop_I_test = Edges.label_prop_I_test\n",
    "    label_prop_I_test = np.reshape(label_prop_I_test,-1)\n",
    "    label_prop_I_test = label_prop_I_test.astype(\"float32\")\n",
    "    if use_raw == 1:\n",
    "        num_use = edge_feature00.shape[1]\n",
    "        trim_id2id = {}\n",
    "        cnt = 0\n",
    "        for i in range(num_use):\n",
    "            if 1 == 1:\n",
    "            #if i % 213 != 1:\n",
    "                if np.sum(edge_feature00[:,i]) > 0:\n",
    "                    trim_id2id.update({cnt:i})\n",
    "                    cnt += 1\n",
    "        edge_feature1 = np.zeros([edge_feature00.shape[0],cnt])\n",
    "        for i in range(cnt):\n",
    "            edge_feature1[:,i] = edge_feature00[:,trim_id2id[i]]\n",
    "        edge_feature0 = copy.copy(edge_feature1)\n",
    "\n",
    "    else:\n",
    "        fileA = train_test_split_time_network + \\\n",
    "            \"_onebp2_edge_feature0_\" + str(feature_dim) + \\\n",
    "            \"_10000_\" + str(threshold_ijyou) + \".npy\"\n",
    "        edge_feature00 = np.load(fileA)\n",
    "        edge_feature0 = copy.copy(edge_feature00)\n",
    "        fileA = train_test_split_time_network + \"_onebp2_edge_feature0_\" + str(feature_dim) + \"_10000_\" + \\\n",
    "            str(threshold_ijyou) +  \"_coef.npy\"\n",
    "        coef0 = np.load(fileA)\n",
    "        fileA = train_test_split_time_network + \"_onebp2_edge_feature0_\" + str(feature_dim) + \"_10000_\" + \\\n",
    "            str(threshold_ijyou) +  \"_label_prop_row_col_moto.npy\"\n",
    "        label_prop_row_col_moto = np.load(fileA)\n",
    "        local_pair2index = {}\n",
    "        for kk in range(label_prop_row_col.shape[0]):\n",
    "            local_pair = str(label_prop_row_col[kk,0]) + \",\" + \\\n",
    "                str(label_prop_row_col[kk,1])\n",
    "            local_pair2index[local_pair] = kk\n",
    "        for kk in range(label_prop_row_col_moto.shape[0]):\n",
    "            row0 = label_prop_row_col_moto[kk,0]\n",
    "            col0 = label_prop_row_col_moto[kk,1]\n",
    "            local_row0 = Edges.id2label_prop_id[row0]\n",
    "            local_col0 = Edges.id2label_prop_id[col0]\n",
    "            local_pair = str(local_row0) + \",\" + str(local_col0)\n",
    "            index = local_pair2index[local_pair]\n",
    "            edge_feature0[index,:] = copy.copy(edge_feature00[kk,:])\n",
    "    # TRAIN parameter\n",
    "    if (pattern_activation == 3) | (pattern_activation == 4):\n",
    "        alpha10 = keisu*np.random.randn(edge_feature0.shape[1],reduce_dimension).astype(\"float32\")\n",
    "        beta10  = keisu*np.random.randn(reduce_dimension).astype(\"float32\")\n",
    "        alpha20 = keisu*np.random.randn(reduce_dimension).astype(\"float32\")\n",
    "        beta20 =  keisu*np.random.randn(1).astype(\"float32\")\n",
    "    elif pattern_activation == 2:\n",
    "        alpha0 = keisu*np.random.randn(edge_feature0.shape[1]).astype(\"float32\")\n",
    "        beta0 = keisu*np.random.randn(1).astype(\"float32\")\n",
    "    else:\n",
    "        alpha0 = np.random.randn(edge_feature0.shape[1]).astype(\"float32\")\n",
    "    ## COMPUTATION GRAPH ##\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if no_gpu == 1:\n",
    "        import os\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" \n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python.framework import ops\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    ops.reset_default_graph()\n",
    "    sess =  tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True))\n",
    "    if (pattern_activation == 3) | (pattern_activation == 4):\n",
    "        alpha1_init = tf.placeholder(shape=alpha10.shape,dtype=tf.float32)\n",
    "        alpha1 = tf.Variable(alpha1_init)\n",
    "        alpha1 = tf.reshape(alpha1,[edge_feature0.shape[1],-1])\n",
    "        beta1_init = tf.placeholder(shape=beta10.shape,dtype=tf.float32)\n",
    "        beta1 = tf.Variable(beta1_init)\n",
    "        beta1 = tf.reshape(beta1,[-1,reduce_dimension])\n",
    "        alpha2_init = tf.placeholder(shape=alpha20.shape,dtype=tf.float32)\n",
    "        alpha2 = tf.Variable(alpha2_init)\n",
    "        alpha2 = tf.reshape(alpha2,[reduce_dimension,-1])\n",
    "        beta2_init = tf.placeholder(shape=beta20.shape,dtype=tf.float32)\n",
    "        beta2 = tf.Variable(beta2_init)\n",
    "    elif (pattern_activation == 2):\n",
    "        alpha_init = tf.placeholder(shape=alpha0.shape,dtype=tf.float32)\n",
    "        alpha = tf.Variable(alpha_init)\n",
    "        alpha = tf.reshape(alpha,[edge_feature0.shape[1],-1])\n",
    "        beta_init = tf.placeholder(shape=beta0.shape,dtype=tf.float32)\n",
    "        beta = tf.Variable(beta_init)\n",
    "    else:\n",
    "        alpha_init = tf.placeholder(shape=alpha0.shape,dtype=tf.float32)\n",
    "        alpha = tf.Variable(alpha_init)    \n",
    "        alpha = tf.reshape(alpha,[num_features,-1])\n",
    "    edge_feature_init = tf.placeholder(tf.float32, shape=edge_feature0.shape)\n",
    "    edge_feature = tf.Variable(edge_feature_init,trainable=False)\n",
    "\n",
    "    # Create label_prop_matrix\n",
    "    if pattern_activation == 3:# Pattern C\n",
    "        middle = tf.sigmoid(tf.matmul(edge_feature,alpha1) + beta1)\n",
    "        edge_weight_data = tf.sigmoid(tf.matmul(middle,alpha2) + beta2)\n",
    "    elif pattern_activation == 4:# Pattern D\n",
    "        middle = tf.nn.relu(tf.matmul(edge_feature,alpha1) + beta1)\n",
    "        edge_weight_data = tf.sigmoid(tf.matmul(middle,alpha2) + beta2) \n",
    "    elif pattern_activation == 1:# Pattern A\n",
    "        edge_weight_data = tf.sigmoid(tf.matmul(edge_feature,alpha))\n",
    "    elif pattern_activation == 2:# Pattern B\n",
    "        edge_weight_data = tf.sigmoid(tf.matmul(edge_feature,alpha) + beta)\n",
    "        \n",
    "    edge_weight_data = tf.reshape(edge_weight_data,[-1])\n",
    "    label_prop_matrix = tf.SparseTensor(indices=label_prop_row_col,values=edge_weight_data,\n",
    "                        dense_shape=tf.constant(core_matrix_shape,tf.int64))    \n",
    "\n",
    "    if correct_norm == 1:\n",
    "        label_prop_matrix_rowsum0 = tf.sparse_reduce_sum(label_prop_matrix, 1)\n",
    "        label_prop_matrix_rowsum1 = label_prop_matrix_rowsum0 + label_prop_I_train\n",
    "        label_prop_matrix_rowsum_inverse = 1.0/label_prop_matrix_rowsum1\n",
    "    if normalize==1:\n",
    "        b = tf.sparse_reduce_sum(label_prop_matrix, axis=1)\n",
    "        label_prop_matrix2 = label_prop_matrix / tf.reshape(b, (-1, 1))\n",
    "\n",
    "    # Inverse A :\n",
    "    if correct_norm == 1:\n",
    "        A_inv = tf.SparseTensor(indices=label_prop_diagonal,\n",
    "                values=label_prop_matrix_rowsum_inverse,\n",
    "                dense_shape=tf.constant(core_matrix_shape,tf.int64))     \n",
    "    else:\n",
    "        A_inv = tf.SparseTensor(indices=label_prop_diagonal,\n",
    "                values=label_prop_inverse_train,\n",
    "                dense_shape=tf.constant(core_matrix_shape,tf.int64))\n",
    "    if initialize == 1:\n",
    "        if one_minus_one == 1:\n",
    "            f = 0*np.random.uniform(bound_left,bound_right,len(y_init_train)) - 1\n",
    "        else:\n",
    "            f = 0*np.random.uniform(bound_left,bound_right,len(y_init_train))\n",
    "    else:\n",
    "        f = np.random.uniform(bound_left,bound_right,len(y_init_train))\n",
    "    f = f.astype(\"float32\")\n",
    "    f = tf.reshape(f,[len(y_init_train),-1])\n",
    "    for i in range(inner_iteration):\n",
    "        f0 = f\n",
    "        if normalize == 1:\n",
    "            tempB = y_init_train + mu * tf.sparse_tensor_dense_matmul(label_prop_matrix2,f0)\n",
    "        else:\n",
    "            tempB = y_init_train + mu * tf.sparse_tensor_dense_matmul(label_prop_matrix,f0)\n",
    "        tempB = tf.reshape(tempB,(len(y_full),-1)) \n",
    "        f = tf.sparse_tensor_dense_matmul(A_inv,tempB,adjoint_a=False,adjoint_b=False,name=None)\n",
    "    \n",
    "    # Retreive test predict\n",
    "    fpre = tf.gather_nd(f,eval_indices_train)\n",
    "    # Regularize\n",
    "    if (pattern_activation == 3) | (pattern_activation == 4):\n",
    "        penalty = lambda_reg*tf.nn.l2_loss(alpha1) + lambda_reg*tf.nn.l2_loss(alpha2)\n",
    "    else:\n",
    "        penalty = lambda_reg*tf.nn.l2_loss(alpha)\n",
    "    # loss\n",
    "    if pattern_loss == 1:\n",
    "        loss = tf.losses.mean_squared_error(fpre,y_full[eval_indices_train_list,0]) + penalty\n",
    "    else:\n",
    "        fpre2 = fpre + tf.constant(0.001)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_full[eval_indices_train_list,0],\n",
    "                    logits=(tf.log(fpre2)))/float(len(eval_indices_train_list)) + penalty\n",
    "    \n",
    "    # Optimizer\n",
    "    my_opt = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "    train_step = my_opt.minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    if (pattern_activation == 3) | (pattern_activation==4):\n",
    "        sess.run(init,feed_dict={alpha1_init:alpha10,beta1_init:beta10,alpha2_init:alpha20,\n",
    "                                 beta2_init:beta20,edge_feature_init:edge_feature0})\n",
    "    elif pattern_activation == 2:    \n",
    "        sess.run(init,feed_dict={alpha_init:alpha0,beta_init:beta0,edge_feature_init:edge_feature0})\n",
    "    else:\n",
    "        sess.run(init,feed_dict={alpha_init:alpha0,edge_feature_init:edge_feature0})\n",
    "  \n",
    "    #otest1,otest2,otest3= sess.run([label_prop_matrix_rowsum0,\n",
    "    #                               label_prop_matrix_rowsum1,\n",
    "    #                               label_prop_matrix_rowsum_inverse])\n",
    "    \n",
    "    # Clear loss tracker\n",
    "    track_loss = []\n",
    "    for itr in range(total_iteration):\n",
    "        _ = sess.run([train_step],feed_dict=None)\n",
    "        if (pattern_activation == 3) | (pattern_activation == 4):\n",
    "            oloss,oalpha1,obeta1,oalpha2,obeta2,ofpre = sess.run([loss,\n",
    "                                        alpha1,beta1,alpha2,beta2,fpre])\n",
    "        elif(pattern_activation == 2):\n",
    "            oloss,oalpha,obeta,ofpre = sess.run([loss,alpha,beta,fpre],feed_dict=None)  \n",
    "        else:\n",
    "            oloss,oalpha,ofpre = sess.run([loss,alpha,fpre],feed_dict=None)   \n",
    "        if (itr % 500 == 0) | (itr == 0):\n",
    "            track_loss.append(oloss)\n",
    "            if verbose == 1:\n",
    "                print(\"Iteration: \" + str(itr) + \" Loss: \" + str(oloss))\n",
    "        if (itr % total_iteration == 0) | (itr == 0):\n",
    "            print(\"Iteration: \" + str(itr) + \" Loss: \" + str(oloss))    \n",
    "    final_train_loss = str(10000000000*oloss)\n",
    "    final_train_loss = final_train_loss[:5]\n",
    "    print(\"Final Loss: \" + str(oloss))\n",
    "\n",
    "    if use_raw != 1:\n",
    "        ## PDP ##\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        if no_gpu == 1:\n",
    "            import os\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" \n",
    "        import tensorflow as tf\n",
    "        from tensorflow.python.framework import ops\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        ops.reset_default_graph()\n",
    "        sess =  tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True))\n",
    "        if (pattern_activation == 3) | (pattern_activation == 4):\n",
    "            alpha1_init = tf.placeholder(shape=alpha10.shape,dtype=tf.float32)\n",
    "            alpha1 = tf.Variable(alpha1_init)\n",
    "            alpha1 = tf.reshape(alpha1,[edge_feature0.shape[1],-1])\n",
    "            beta1_init = tf.placeholder(shape=beta10.shape,dtype=tf.float32)\n",
    "            beta1 = tf.Variable(beta1_init)\n",
    "            beta1 = tf.reshape(beta1,[-1,reduce_dimension])\n",
    "            alpha2_init = tf.placeholder(shape=alpha20.shape,dtype=tf.float32)\n",
    "            alpha2 = tf.Variable(alpha2_init)\n",
    "            alpha2 = tf.reshape(alpha2,[reduce_dimension,-1])\n",
    "            beta2_init = tf.placeholder(shape=beta20.shape,dtype=tf.float32)\n",
    "            beta2 = tf.Variable(beta2_init)\n",
    "        elif (pattern_activation == 2):\n",
    "            alpha_init = tf.placeholder(shape=alpha0.shape,dtype=tf.float32)\n",
    "            alpha = tf.Variable(alpha_init)\n",
    "            alpha = tf.reshape(alpha,[edge_feature0.shape[1],-1])\n",
    "            beta_init = tf.placeholder(shape=beta0.shape,dtype=tf.float32)\n",
    "            beta = tf.Variable(beta_init)\n",
    "        else:\n",
    "            alpha_init = tf.placeholder(shape=alpha0.shape,dtype=tf.float32)\n",
    "            alpha = tf.Variable(alpha_init)    \n",
    "            alpha = tf.reshape(alpha,[num_features,-1])\n",
    "        edge_feature2 = tf.placeholder(tf.float32, shape=edge_feature0.shape)\n",
    "        if pattern_activation == 3:# Pattern C\n",
    "            middle = tf.sigmoid(tf.matmul(edge_feature2,alpha1) + beta1)\n",
    "            edge_weight_data = tf.sigmoid(tf.matmul(middle,alpha2) + beta2)\n",
    "        elif pattern_activation == 4:# Pattern D\n",
    "            middle = tf.nn.relu(tf.matmul(edge_feature2,alpha1) + beta1)\n",
    "            edge_weight_data = tf.sigmoid(tf.matmul(middle,alpha2) + beta2) \n",
    "        elif pattern_activation == 1:# Pattern A\n",
    "            edge_weight_data = tf.sigmoid(tf.matmul(edge_feature2,alpha))\n",
    "        elif pattern_activation == 2:# Pattern B\n",
    "            edge_weight_data = tf.sigmoid(tf.matmul(edge_feature2,alpha) + beta)\n",
    "        edge_weight_data = tf.reshape(edge_weight_data,[-1])\n",
    "        temp = tf.reduce_mean(edge_weight_data)  \n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        if (pattern_activation == 3) | (pattern_activation==4):\n",
    "            sess.run(init,feed_dict={alpha1_init:oalpha1,beta1_init:obeta1[0,:],\n",
    "                                     alpha2_init:oalpha2[:,0],beta2_init:obeta2,\n",
    "                                     #edge_feature_init:edge_feature0\n",
    "                                    })\n",
    "        elif pattern_activation == 2:    \n",
    "            sess.run(init,feed_dict={alpha_init:oalpha[:,0],beta_init:obeta,\n",
    "                                     #edge_feature_init:edge_feature0\n",
    "                                    })\n",
    "        else:\n",
    "            sess.run(init,feed_dict={alpha_init:oalpha[:,0],#edge_feature_init:edge_feature0\n",
    "                                    })\n",
    "    \n",
    "        output = []\n",
    "        adaptive_cut = 1\n",
    "        num_divide = 5\n",
    "        cut  = 0.2\n",
    "        saisyo = 0\n",
    "        num_itr = edge_feature0.shape[1]\n",
    "        total_calc = edge_feature0.shape[1] * num_divide\n",
    "        kk_list = list(range(total_calc))\n",
    "        #random.shuffle(kk_list)\n",
    "        for kk in kk_list:\n",
    "            if kk % 100000 == 0:\n",
    "                print(kk)\n",
    "            col_num00 = math.floor(kk / num_divide)\n",
    "            col_num0 = np.asarray(col_num00)\n",
    "            col_num0 = col_num0.astype(\"int64\")\n",
    "            if adaptive_cut == 1:\n",
    "                #if 1 == 1:\n",
    "                if kk % num_divide == 0:\n",
    "                    saidai = np.percentile(edge_feature0[:,col_num00],100)\n",
    "                    saisyo = np.percentile(edge_feature0[:,col_num00],0)\n",
    "                    cut = (saidai - saisyo) / (num_divide - 1)\n",
    "            place = (kk) % num_divide\n",
    "            cut_num00 = (kk) % num_divide * cut + saisyo\n",
    "            cut_num0 = np.asarray(cut_num00)\n",
    "            cut_num0 = cut_num0.astype(\"float32\")\n",
    "            edge_feature_in = copy.copy(edge_feature0)\n",
    "            edge_feature_in[:,col_num00] = cut_num00\n",
    "            otemp = sess.run([temp],feed_dict={edge_feature2:edge_feature_in})\n",
    "            output.append([col_num00,place,cut_num00,otemp[0]])\n",
    "        dfPDP = pd.DataFrame(output)\n",
    "        dfPDP.columns = [\"column\",\"place\",\"cut\",\"value\"]\n",
    "        dfPDP.sort_values(by=[\"column\",\"place\"],ascending=[\"True\",\"True\"],inplace=True)\n",
    "        dfPDP2 = dfPDP.groupby(\"column\")[\"value\"].transform(\"std\").rename(\"std\")\n",
    "        dfPDP3 = pd.concat([dfPDP,dfPDP2],axis=1)\n",
    "        dfPDP3.drop_duplicates(subset=\"std\",inplace=True)\n",
    "        dfPDP3 = dfPDP3.sort_values(by=\"std\",ascending=False)\n",
    "        #print(dfPDP3.head(10))\n",
    "        dfPDP[\"sabun\"] = dfPDP[\"value\"] - dfPDP[\"value\"].shift(num_divide-1)\n",
    "        cond = dfPDP[\"place\"] == max(dfPDP[\"place\"])\n",
    "        dfPDP4 = dfPDP.loc[cond]\n",
    "        #plt.hist(dfPDP4[\"sabun\"])\n",
    "        #plt.show()\n",
    "        dfPDP4 = dfPDP4.sort_values(by=\"sabun\",ascending=False)\n",
    "        #print(dfPDP4.head(10))\n",
    "        #print(dfPDP4.tail(10))\n",
    "        \n",
    "        # PDP\n",
    "        file_pdp = \"pdp_20170201/\" + train_test_split_time + \"_\" + str(lag_days) + \\\n",
    "            \"_\" + use_label + \"_onebp_\" + str(pattern_activation) + \"_\" + \\\n",
    "            str(reduce_dimension) + \"_\" + str(feature_dim) + \"_\" + \\\n",
    "            str(threshold_ijyou) + \"_\" + final_train_loss + \"_\" + \\\n",
    "            str(initialize) + \"_\" + str(correct_norm) + \"_pdp.csv\"\n",
    "        with open(file_pdp,\"w\") as fwrite:\n",
    "            out = \"column,std,sabun\\n\"\n",
    "            fwrite.write(out)\n",
    "            for i in range(len(dfPDP3)):\n",
    "                cond = dfPDP4[\"column\"] == dfPDP3[\"column\"].iloc[i]\n",
    "                temp = dfPDP4[\"sabun\"].loc[cond]\n",
    "                out = str(dfPDP3[\"column\"].iloc[i]) + \",\" + \\\n",
    "                str(dfPDP3[\"std\"].iloc[i]) + \",\" + \\\n",
    "                str(float(temp)) + \"\\n\"\n",
    "                fwrite.write(out)\n",
    "\n",
    "    # EVALUATION #\n",
    "    ops.reset_default_graph()\n",
    "    sess =  tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True))\n",
    "    if (pattern_activation == 3) | (pattern_activation == 4):\n",
    "        alpha1_init = tf.placeholder(shape=alpha10.shape,dtype=tf.float32)\n",
    "        alpha1 = tf.Variable(alpha1_init)\n",
    "        alpha1 = tf.reshape(alpha1,[edge_feature0.shape[1],-1])\n",
    "        beta1_init = tf.placeholder(shape=beta10.shape,dtype=tf.float32)\n",
    "        beta1 = tf.Variable(beta1_init)\n",
    "        beta1 = tf.reshape(beta1,[-1,reduce_dimension])\n",
    "        alpha2_init = tf.placeholder(shape=alpha20.shape,dtype=tf.float32)\n",
    "        alpha2 = tf.Variable(alpha2_init)\n",
    "        alpha2 = tf.reshape(alpha2,[reduce_dimension,-1])\n",
    "        beta2_init = tf.placeholder(shape=beta20.shape,dtype=tf.float32)\n",
    "        beta2 = tf.Variable(beta2_init)\n",
    "    elif (pattern_activation == 2):\n",
    "        alpha_init = tf.placeholder(shape=alpha0.shape,dtype=tf.float32)\n",
    "        alpha = tf.Variable(alpha_init)\n",
    "        alpha = tf.reshape(alpha,[edge_feature0.shape[1],-1])\n",
    "        beta_init = tf.placeholder(shape=beta0.shape,dtype=tf.float32)\n",
    "        beta = tf.Variable(beta_init)\n",
    "    else:\n",
    "        alpha_init = tf.placeholder(shape=alpha0.shape,dtype=tf.float32)\n",
    "        alpha = tf.Variable(alpha_init)    \n",
    "        alpha = tf.reshape(alpha,[num_features,-1])\n",
    "    edge_feature_init = tf.placeholder(tf.float32, shape=edge_feature0.shape)\n",
    "    edge_feature = tf.Variable(edge_feature_init,trainable=False)\n",
    "    \n",
    "    # Create label_prop_matrix\n",
    "    if pattern_activation == 3:# Pattern C\n",
    "        middle = tf.sigmoid(tf.matmul(edge_feature,alpha1) + beta1)\n",
    "        edge_weight_data = tf.sigmoid(tf.matmul(middle,alpha2) + beta2)\n",
    "    elif pattern_activation == 4:# Pattern D\n",
    "        middle = tf.nn.relu(tf.matmul(edge_feature,alpha1) + beta1)\n",
    "        edge_weight_data = tf.sigmoid(tf.matmul(middle,alpha2) + beta2) \n",
    "    elif pattern_activation == 1:# Pattern A\n",
    "        edge_weight_data = tf.sigmoid(tf.matmul(edge_feature,alpha))\n",
    "    elif pattern_activation == 2:# Pattern B\n",
    "        edge_weight_data = tf.sigmoid(tf.matmul(edge_feature,alpha) + beta)\n",
    "    edge_weight_data = tf.reshape(edge_weight_data,[-1])\n",
    "    label_prop_matrix = tf.SparseTensor(indices=label_prop_row_col,values=edge_weight_data,\n",
    "                        dense_shape=tf.constant(core_matrix_shape,tf.int64))    \n",
    "    if normalize==1:\n",
    "        b = tf.sparse_reduce_sum(label_prop_matrix, axis=1)\n",
    "        label_prop_matrix2 = label_prop_matrix / tf.reshape(b, (-1, 1))\n",
    "    if correct_norm == 1:\n",
    "        label_prop_matrix_rowsum0 = tf.sparse_reduce_sum(label_prop_matrix, 1)\n",
    "        label_prop_matrix_rowsum1 = label_prop_matrix_rowsum0 + label_prop_I_test\n",
    "        label_prop_matrix_rowsum_inverse = 1.0/label_prop_matrix_rowsum1\n",
    "    \n",
    "    # Inverse A\n",
    "    if correct_norm == 1:\n",
    "        A_inv = tf.SparseTensor(indices=label_prop_diagonal,\n",
    "                    values=label_prop_matrix_rowsum_inverse,\n",
    "                    dense_shape=tf.constant(core_matrix_shape,tf.int64))\n",
    "    else: \n",
    "        A_inv = tf.SparseTensor(indices=label_prop_diagonal,\n",
    "                    values=label_prop_inverse_test,\n",
    "                    dense_shape=tf.constant(core_matrix_shape,tf.int64))\n",
    "    if initialize == 1:\n",
    "        f = 0*np.random.uniform(bound_left,bound_right,len(y_init_test))\n",
    "    else:\n",
    "        f = np.random.uniform(bound_left,bound_right,len(y_init_test))\n",
    "    f = f.astype(\"float32\")\n",
    "    f = tf.reshape(f,[len(y_init_test),-1])\n",
    "    for i in range(inner_iteration):\n",
    "        f0 = f\n",
    "        if normalize == 1:\n",
    "            tempB = y_init_test + mu * tf.sparse_tensor_dense_matmul(label_prop_matrix2,f0)\n",
    "        else:\n",
    "            tempB = y_init_test + mu * tf.sparse_tensor_dense_matmul(label_prop_matrix,f0)\n",
    "        tempB = tf.reshape(tempB,(len(y_full),-1)) \n",
    "        f = tf.sparse_tensor_dense_matmul(A_inv,tempB,adjoint_a=False,adjoint_b=False,name=None)\n",
    "    # Retreive test predict\n",
    "    fpre = tf.gather_nd(f,eval_indices_test)\n",
    "    # Regularize\n",
    "    if (pattern_activation == 3) | (pattern_activation == 4):\n",
    "        penalty = lambda_reg * tf.reduce_sum(tf.abs(alpha1))/float(num_features)\n",
    "    else:\n",
    "        penalty = lambda_reg * tf.reduce_sum(tf.abs(alpha))/float(num_features)\n",
    "    # loss\n",
    "    if pattern_loss == 1:\n",
    "        loss = tf.losses.mean_squared_error(fpre,y_full[eval_indices_test_list,0]) + penalty\n",
    "    else:\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_full[eval_indices_test_list,0],\n",
    "                    logits=(-fpre))/float(len(eval_indices_test_list)) + penalty\n",
    "    # Optimizer\n",
    "    my_opt = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "    train_step = my_opt.minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    if (pattern_activation == 3) | (pattern_activation==4):\n",
    "        sess.run(init,feed_dict={alpha1_init:oalpha1,beta1_init:obeta1[0,:],\n",
    "                alpha2_init:oalpha2[:,0],beta2_init:obeta2,\n",
    "                edge_feature_init:edge_feature0})\n",
    "    elif pattern_activation == 2:    \n",
    "        sess.run(init,feed_dict={alpha_init:oalpha[:,0],beta_init:obeta,\n",
    "                edge_feature_init:edge_feature0})\n",
    "    else:\n",
    "        sess.run(init,feed_dict={alpha_init:oalpha[:,0],edge_feature_init:edge_feature0})\n",
    "    for itr in range(1):\n",
    "        if (pattern_activation == 3) | (pattern_activation == 4):\n",
    "            oloss,oalpha1,obeta1,oalpha2,obeta2,ofpre,out_edge_weight = sess.run([loss,\n",
    "                                        alpha1,beta1,alpha2,beta2,fpre,edge_weight_data])\n",
    "        elif(pattern_activation == 2):\n",
    "            oloss,oalpha,obeta,ofpre,out_edge_weight = sess.run([loss,alpha,beta,fpre,edge_weight_data],feed_dict=None)  \n",
    "        else:\n",
    "            oloss,oalpha,ofpre,out_edge_weight = sess.run([loss,alpha,fpre,edge_weight_data],feed_dict=None)   \n",
    "\n",
    "    # Evaluate\n",
    "    ypre = ofpre\n",
    "    ylab= y_full[eval_indices_test_list]\n",
    "    # Calculate\n",
    "    random_guess = np.sum(ylab)/len(ylab)\n",
    "    precision, recall, thresholds = precision_recall_curve(ylab,ypre)\n",
    "    area = auc(recall, precision)\n",
    "    print(\"Random Prediction: \" + str(random_guess) + \\\n",
    "          \" AP: \" + str(average_precision_score(ylab, ypre, average='weighted')) + \\\n",
    "          \" ROC: \" + str(roc_auc_score(ylab, ypre)))\n",
    "\n",
    "    file_pdf = \"figure_20170201/\" + train_test_split_time + \"_onebp_\" + \\\n",
    "            use_label + \"_\" + str(use_raw) + \".pdf\"\n",
    "    params = {\n",
    "       'axes.labelsize': 14,\n",
    "       'font.size': 14,\n",
    "       'legend.fontsize': 14,\n",
    "       'xtick.labelsize': 14,\n",
    "       'ytick.labelsize': 14,\n",
    "       'text.usetex': False,\n",
    "       'figure.figsize': [6, 4]\n",
    "    }\n",
    "    plt.rcParams.update(params)\n",
    "    plt.xlabel('Score')\n",
    "    plt.ylabel('True Label')\n",
    "    yint = range(0, 2)\n",
    "    plt.yticks(yint)\n",
    "    plt.plot(ypre,ylab,marker=\"+\",markersize=10,\n",
    "             markeredgewidth=1.5,linewidth=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_pdf)\n",
    "    plt.show()\n",
    "    \n",
    "    if use_raw == 1:\n",
    "        moto_id_list = []\n",
    "        for kk in range(len(eval_indices_test_list)):\n",
    "            local_id = eval_indices_test_list[kk]\n",
    "            moto_id = Edges.label_prop_id2id[local_id]\n",
    "            moto_id_list.append(moto_id)\n",
    "        dfScore= pd.DataFrame({\"local_id\":eval_indices_test_list,\"id\":moto_id_list,\n",
    "                               \"score\":ypre,\"label\":ylab[:,0]})\n",
    "        dfScore.sort_values(by=\"score\",ascending=False,inplace=True)\n",
    "        file_score = \"score_20170201/\" + train_test_split_time + \"_\" + str(lag_days) + \\\n",
    "            \"_\" + use_label + \"_onebp_\" + str(pattern_activation) + \"_\" + \\\n",
    "            str(reduce_dimension) + \"_\" + str(feature_dim) + \"_\" + \\\n",
    "            str(threshold_ijyou) + \"_\" + final_train_loss + \"_\" + \\\n",
    "            str(initialize) + \"_\" + str(correct_norm) + \"_score.csv\"\n",
    "        dfScore.to_csv(file_score,index=False)  \n",
    "\n",
    "        # Output file\n",
    "        file_edge = \"score_20170201/\" + train_test_split_time + \"_\" + str(lag_days) + \"_\" + \\\n",
    "            use_label + \"_onebp_\" + str(pattern_activation) + \"_\" + str(reduce_dimension) + \\\n",
    "            \"_\" + str(feature_dim) + \"_\" + str(threshold_ijyou) + \\\n",
    "            \"_\" + final_train_loss + \"_\" + str(initialize) + \\\n",
    "            \"_\" + str(correct_norm) + \"_edge_weight.npy\"\n",
    "        np.save(file_edge,out_edge_weight)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LP-path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 == 0:\n",
    "    use_label0 = \"Management\"\n",
    "    threshold_ijyou = 1\n",
    "    total_iteration = 10000\n",
    "    pattern_activation = 3\n",
    "    reduce_dimension = 30\n",
    "    initialize = 1\n",
    "    cut = 10000\n",
    "    zero_one = 1\n",
    "    lag_days = 31\n",
    "    path_threshold = 3000\n",
    "    keisu = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Path2(use_label0,threshold_ijyou,total_iteration,temp_date0,\n",
    "          normalize,pattern_activation,reduce_dimension,initialize,cut,\n",
    "          zero_one,lag_days,keisu,path_threshold):\n",
    "    serial_hantei = 1\n",
    "    divide_calc = 1\n",
    "    which_feature = \"path\"\n",
    "    \n",
    "    # START COMMON: 20180920 #\n",
    "    use_label = re.sub(\"/\",\"-\",use_label0)\n",
    "    Edges.InitializeSparseCoreMatrix(1)\n",
    "    temp_date1 = temp_date0 - datetime.timedelta(days=lag_days)\n",
    "    temp_date2 = temp_date0 + datetime.timedelta(days=50000)\n",
    "    train_test_split_time = datetime.datetime.strftime(temp_date0,\"%Y-%m-%d\")\n",
    "    train_deve_split_time = datetime.datetime.strftime(temp_date1,\"%Y-%m-%d\")\n",
    "    test_end_split_time = datetime.datetime.strftime(temp_date2,\"%Y-%m-%d\")\n",
    "    print(\"train test:\" + str(train_test_split_time) + \",train deve:\" + str(train_deve_split_time))\n",
    "    file_object = header + \"_\" + use_label + \".pkl\"\n",
    "    with open(file_object, mode='rb') as f:\n",
    "        objects = pickle.load(f)\n",
    "    train_positive0 = objects[0]\n",
    "    train_positive_time_list = objects[1]\n",
    "    test_positive0  = objects[2]\n",
    "    test_positive_time_list = objects[3]\n",
    "    print(use_label + \" Num Train: \" + str(len(train_positive0)) + \" Num Test: \" + str(len(test_positive0)))\n",
    "    Edges.ClearTrainPositiveTime()\n",
    "    for i in range(len(train_positive_time_list)):\n",
    "        Edges.train_positive_time.append(train_positive_time_list[i])\n",
    "    # END COMMON: 20180920 #\n",
    "    \n",
    "    # FILE NAME\n",
    "    file_path = \"Path_Count_4_\" + str(threshold_ijyou) + \".csv\"\n",
    "    file_path_2 = \"Path_Feature_Sparse_\" + str(threshold_ijyou) + \".csv\"\n",
    "\n",
    "    # KOKO \n",
    "    create_path2count = 0\n",
    "    if create_path2count == 1:\n",
    "        Edges.CreatePath2Count2(train_positive0,\n",
    "            train_deve_split_time,test_positive0,threshold_ijyou,\n",
    "            max_depth,reduce_dimension,mu,epsilon)\n",
    "        dfP = pd.DataFrame(list(Edges.path2count.items()),columns=['path','count'])\n",
    "        dfP.sort_values(\"count\",ascending=False,inplace=True)\n",
    "        file_path = \"Path_Count_4\" + \".csv\"\n",
    "        dfP.to_csv(file_path,index=False,columns=[\"path\",\"count\"])\n",
    "        \n",
    "    if which_feature == \"path\":\n",
    "        Edges.CreatePathMatrix3(train_positive0,\n",
    "            train_deve_split_time,test_positive0,threshold_ijyou,\n",
    "            max_depth,reduce_dimension,mu,epsilon,file_path,path_threshold,\n",
    "            zero_one)\n",
    "        \n",
    "        label_prop_row_col = []\n",
    "        pair2index = {}\n",
    "        # moto no ichi wo ireru\n",
    "        for i in range(len(Edges.label_prop_row)):\n",
    "            pair1 = str(Edges.label_prop_row[i]) + \",\" + str(Edges.label_prop_col[i])\n",
    "            pair2index.update({pair1:i})\n",
    "            label_prop_row_col.append([Edges.label_prop_row[i],\n",
    "                                       Edges.label_prop_col[i]])\n",
    "        moto_edge_id_list = []\n",
    "        temp = sorted(label_prop_row_col,key=lambda l:l[1])\n",
    "        label_prop_row_col = sorted(temp,key=lambda l:l[0])\n",
    "        label_prop_row_col = np.asarray(label_prop_row_col,np.int64) \n",
    "        # Since label_prop_row_col is sorted i in sorted_edge_id\n",
    "        # while place is moto_edge_id\n",
    "        for i in range(len(Edges.label_prop_row)):\n",
    "            pair1 = str(label_prop_row_col[i,0]) + \",\" + \\\n",
    "                    str(label_prop_row_col[i,1])\n",
    "            place = pair2index[pair1]\n",
    "            moto_edge_id_list.append(place)\n",
    "        moto_edge_id0 = np.asarray(moto_edge_id_list)\n",
    "        moto_edge_id0 = moto_edge_id0.astype(\"int64\")\n",
    "        Edges.ReorganizePathMatrix(file_path_2,0,moto_edge_id0)\n",
    "    else: \n",
    "        pass\n",
    "    print(str(len(Edges.label_prop_id2id)) + \",\" + str(Edges.label_prop_counter))\n",
    "    dftemp = pd.read_csv(file_path_2)\n",
    "    dftemp.columns = [\"row\",\"col\",\"value\"]    \n",
    "    dftemp = dftemp.sort_values([\"row\",\"col\"],ascending=[True,True],inplace=False)\n",
    "    print(len(dftemp))\n",
    "    print(np.min(dftemp[\"col\"]))\n",
    "    print(np.max(dftemp[\"col\"]))\n",
    "    \n",
    "    edge_feature_index0 = dftemp.iloc[:,0:2].values\n",
    "    edge_feature_value0 = dftemp.iloc[:,2].values\n",
    "    edge_feature_index0 = edge_feature_index0.astype(\"int64\")\n",
    "    edge_feature_value0 = edge_feature_value0.astype(\"float32\")\n",
    "    num_edges = len(label_prop_row_col)\n",
    "    num_features = max(dftemp[\"col\"])+1\n",
    "    print(\"num edges:\" + str(num_edges) + \" num features:\" + str(num_features))\n",
    "    \n",
    "    if 1==0:#sanity check\n",
    "        pair1 = str(label_prop_row_col[i,0]) + \",\" + \\\n",
    "                str(label_prop_row_col[i,1])\n",
    "        pair2 = str(label_prop_row_col[i,1]) + \",\" + \\\n",
    "                str(label_prop_row_col[i,0])    \n",
    "        print(pair2index[pair1])\n",
    "        print(pair2index[pair2])\n",
    "        cond = dftemp[\"row\"] == Edges.moto_edge_id2sorted_edge_id[pair2index[pair1]]\n",
    "        print(dftemp.loc[cond])\n",
    "        cond = dftemp[\"row\"] == Edges.moto_edge_id2sorted_edge_id[pair2index[pair2]]\n",
    "        print(dftemp.loc[cond])\n",
    "        \n",
    "    if divide_calc == 0:\n",
    "        pass\n",
    "    else:\n",
    "        max_row = max(dftemp[\"row\"]) + 1\n",
    "        cut1 = math.floor(max_row/5)\n",
    "        cut2 = 2*cut1\n",
    "        cut3 = 3*cut1\n",
    "        cut4 = 4*cut1\n",
    "        cut5 = max_row\n",
    "        num_edges1 = cut1\n",
    "        num_edges2 = cut2 - cut1\n",
    "        num_edges3 = cut3 - cut2\n",
    "        num_edges4 = cut4 - cut3\n",
    "        num_edges5 = cut5 - cut4\n",
    "        cond = dftemp[\"row\"] < cut1\n",
    "        dftemp1 = dftemp.loc[cond]\n",
    "        dftemp1 = dftemp1.sort_values([\"row\",\"col\"],\n",
    "                ascending=[True,True],inplace=False)\n",
    "        edge_feature_index10 = dftemp1.iloc[:,0:2].values\n",
    "        edge_feature_value10 = dftemp1.iloc[:,2].values\n",
    "        edge_feature_index10 = edge_feature_index10.astype(\"int64\")\n",
    "        edge_feature_value10 = edge_feature_value10.astype(\"float32\")\n",
    "        cond = ( cut1 <= dftemp[\"row\"] ) & ( dftemp[\"row\"] < cut2 )\n",
    "        dftemp2 = dftemp.loc[cond]\n",
    "        dftemp2 = dftemp2.sort_values([\"row\",\"col\"],ascending=[True,True],inplace=False)\n",
    "        dftemp2[\"row\"] = dftemp2[\"row\"] - cut1\n",
    "        edge_feature_index20 = dftemp2.iloc[:,0:2].values\n",
    "        edge_feature_value20 = dftemp2.iloc[:,2].values\n",
    "        edge_feature_index20 = edge_feature_index20.astype(\"int64\")\n",
    "        edge_feature_value20 = edge_feature_value20.astype(\"float32\")\n",
    "        cond = ( cut2 <= dftemp[\"row\"] ) & ( dftemp[\"row\"] < cut3 )\n",
    "        dftemp3 = dftemp.loc[cond]\n",
    "        dftemp3 = dftemp3.sort_values([\"row\",\"col\"],ascending=[True,True],inplace=False)\n",
    "        dftemp3[\"row\"] = dftemp3[\"row\"] - cut2\n",
    "        edge_feature_index30 = dftemp3.iloc[:,0:2].values\n",
    "        edge_feature_value30 = dftemp3.iloc[:,2].values\n",
    "        edge_feature_index30 = edge_feature_index30.astype(\"int64\")\n",
    "        edge_feature_value30 = edge_feature_value30.astype(\"float32\")\n",
    "        cond = ( cut3 <= dftemp[\"row\"] ) & ( dftemp[\"row\"] < cut4 )\n",
    "        dftemp4 = dftemp.loc[cond]\n",
    "        dftemp4 = dftemp4.sort_values([\"row\",\"col\"],ascending=[True,True],inplace=False)\n",
    "        dftemp4[\"row\"] = dftemp4[\"row\"] - cut3\n",
    "        edge_feature_index40 = dftemp4.iloc[:,0:2].values\n",
    "        edge_feature_value40 = dftemp4.iloc[:,2].values\n",
    "        edge_feature_index40 = edge_feature_index40.astype(\"int64\")\n",
    "        edge_feature_value40 = edge_feature_value40.astype(\"float32\")\n",
    "        cond = ( cut4 <= dftemp[\"row\"] ) \n",
    "        dftemp5 = dftemp.loc[cond]\n",
    "        dftemp5 = dftemp5.sort_values([\"row\",\"col\"],ascending=[True,True],inplace=False)\n",
    "        dftemp5[\"row\"] = dftemp5[\"row\"] - cut4\n",
    "        edge_feature_index50 = dftemp5.iloc[:,0:2].values\n",
    "        edge_feature_value50 = dftemp5.iloc[:,2].values\n",
    "        edge_feature_index50 = edge_feature_index50.astype(\"int64\")\n",
    "        edge_feature_value50 = edge_feature_value50.astype(\"float32\")\n",
    "        \n",
    "    # START COMMON: 20180920 #\n",
    "    y_init_train = Edges.y_init_train\n",
    "    y_init_train = np.reshape(y_init_train,[-1,1])\n",
    "    y_init_train = y_init_train.astype(\"float32\")\n",
    "    y_init_test = Edges.y_init_test\n",
    "    y_init_test = np.reshape(y_init_test,[-1,1])\n",
    "    y_init_test = y_init_test.astype(\"float32\")\n",
    "    y_full = Edges.y_full\n",
    "    y_full = np.reshape(y_full,[-1,1])\n",
    "    y_full = y_full.astype(\"float32\")\n",
    "    print(str(np.sum(y_init_train)) + \",\" + str(np.sum(y_init_test)) + \",\" + str(np.sum(y_full)) )\n",
    "    #if one_minus_one == 1:\n",
    "    #    y_init_train = 2*y_init_train - 1\n",
    "    #    y_init_test = 2*y_init_test - 1\n",
    "    #    y_full = 2*y_full - 1\n",
    "    eval_indices_train,eval_indices_train_list = ([] for _ in range(2))\n",
    "    for i in range(len(Edges.eval_indices_train)):\n",
    "        eval_indices_train_list.append(np.int64(Edges.eval_indices_train[i]))\n",
    "        eval_indices_train.append([np.int64(Edges.eval_indices_train[i]),0])\n",
    "    eval_indices_train = np.asarray(eval_indices_train,np.int64)\n",
    "    eval_indices_test,eval_indices_test_list = ([] for _ in range(2))\n",
    "    for i in range(len(Edges.eval_indices_test)):\n",
    "        eval_indices_test_list.append(np.int64(Edges.eval_indices_test[i]))\n",
    "        eval_indices_test.append([np.int64(Edges.eval_indices_test[i]),0])\n",
    "    eval_indices_test = np.asarray(eval_indices_test,np.int64)\n",
    "    print(str(len(eval_indices_train_list)) + \",\" + str(len(eval_indices_test_list)))\n",
    "    label_prop_diagonal = []\n",
    "    for i in range(len(y_full)):\n",
    "        label_prop_diagonal.append([np.int64(i),np.int64(i)])\n",
    "    label_prop_diagonal = np.asarray(label_prop_diagonal,np.int64)\n",
    "    label_prop_inverse_train = Edges.label_prop_inverse_train\n",
    "    label_prop_inverse_train = np.reshape(label_prop_inverse_train,-1)\n",
    "    label_prop_inverse_train = label_prop_inverse_train.astype(\"float32\")\n",
    "    label_prop_inverse_test = Edges.label_prop_inverse_test\n",
    "    label_prop_inverse_test = np.reshape(label_prop_inverse_test ,-1)\n",
    "    label_prop_inverse_test = label_prop_inverse_test.astype(\"float32\")\n",
    "    core_matrix_shape = [len(y_full),len(y_full)]\n",
    "    core_matrix_shape = np.array(core_matrix_shape, dtype=np.int64)\n",
    "    #num_edges = edge_feature0.shape[0]\n",
    "    #num_features = edge_feature0.shape[1]  \n",
    "    # END COMMON: 20180920 #\n",
    "    \n",
    "    num_edges = len(label_prop_row_col)\n",
    "    num_features = max(dftemp[\"col\"])+1\n",
    "    print(\"num edges:\" + str(num_edges) + \" num features:\" + str(num_features))\n",
    "    \n",
    "    # TRAIN parameter\n",
    "    if (pattern_activation == 3) | (pattern_activation == 4):\n",
    "        alpha10 = keisu*np.random.randn(num_features,reduce_dimension).astype(\"float32\")\n",
    "        beta10  = keisu*np.random.randn(reduce_dimension).astype(\"float32\")\n",
    "        alpha20 = keisu*np.random.randn(reduce_dimension).astype(\"float32\")\n",
    "        beta20 =  keisu*np.random.randn(1).astype(\"float32\")\n",
    "    elif pattern_activation == 2:\n",
    "        alpha0 = keisu*np.random.randn(num_features).astype(\"float32\")\n",
    "        beta0 = keisu*np.random.randn(1).astype(\"float32\")\n",
    "    else:\n",
    "        alpha0 = np.random.randn(num_features).astype(\"float32\")\n",
    "    \n",
    "    #### COMPUTATION GRAPH ####\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if no_gpu == 1:\n",
    "        import os\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" \n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python.framework import ops\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    # Tensorflow calculation\n",
    "    ops.reset_default_graph()\n",
    "    sess =  tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True))\n",
    "    if (pattern_activation == 3) | (pattern_activation == 4):\n",
    "        alpha1_init = tf.placeholder(shape=alpha10.shape,dtype=tf.float32)\n",
    "        alpha1 = tf.Variable(alpha1_init)\n",
    "        alpha1 = tf.reshape(alpha1,[num_features,-1])\n",
    "        beta1_init = tf.placeholder(shape=beta10.shape,dtype=tf.float32)\n",
    "        beta1 = tf.Variable(beta1_init)\n",
    "        beta1 = tf.reshape(beta1,[-1,reduce_dimension])\n",
    "        alpha2_init = tf.placeholder(shape=alpha20.shape,dtype=tf.float32)\n",
    "        alpha2 = tf.Variable(alpha2_init)\n",
    "        alpha2 = tf.reshape(alpha2,[reduce_dimension,-1])\n",
    "        beta2_init = tf.placeholder(shape=beta20.shape,dtype=tf.float32)\n",
    "        beta2 = tf.Variable(beta2_init)\n",
    "    elif (pattern_activation == 2):\n",
    "        alpha_init = tf.placeholder(shape=alpha0.shape,dtype=tf.float32)\n",
    "        alpha = tf.Variable(alpha_init,trainable=True)\n",
    "        alpha = tf.reshape(alpha,[num_features,-1])\n",
    "        beta_init = tf.placeholder(shape=beta0.shape,dtype=tf.float32)\n",
    "        beta = tf.Variable(beta_init,trainable=True)\n",
    "    else:\n",
    "        alpha_init = tf.placeholder(shape=alpha0.shape,dtype=tf.float32)\n",
    "        alpha = tf.Variable(alpha_init)    \n",
    "        alpha = tf.reshape(alpha,[num_features,-1])\n",
    "    if divide_calc == 0:\n",
    "        edge_feature_index_init = tf.placeholder(dtype=tf.int64,shape=edge_feature_index0.shape)\n",
    "        edge_feature_index = tf.Variable(edge_feature_index_init,trainable=False)\n",
    "        edge_feature_value_init = tf.placeholder(dtype=tf.float32,shape=edge_feature_value0.shape)\n",
    "        edge_feature_value = tf.Variable(edge_feature_value_init,trainable=False)\n",
    "        edge_feature = tf.SparseTensor(indices=edge_feature_index,values=edge_feature_value,\n",
    "                                       dense_shape=[num_edges,num_features])\n",
    "    else:\n",
    "        edge_feature_index1_init = tf.placeholder(dtype=tf.int64,shape=edge_feature_index10.shape)\n",
    "        edge_feature_index1 = tf.Variable(edge_feature_index1_init,trainable=False)\n",
    "        edge_feature_value1_init = tf.placeholder(dtype=tf.float32,shape=edge_feature_value10.shape)\n",
    "        edge_feature_value1 = tf.Variable(edge_feature_value1_init,trainable=False)\n",
    "        edge_feature1 = tf.SparseTensor(indices=edge_feature_index1,values=edge_feature_value1,\n",
    "                                       dense_shape=[num_edges1,num_features])\n",
    "        edge_feature_index2_init = tf.placeholder(dtype=tf.int64,shape=edge_feature_index20.shape)\n",
    "        edge_feature_index2 = tf.Variable(edge_feature_index2_init,trainable=False)\n",
    "        edge_feature_value2_init = tf.placeholder(dtype=tf.float32,shape=edge_feature_value20.shape)\n",
    "        edge_feature_value2 = tf.Variable(edge_feature_value2_init,trainable=False)\n",
    "        edge_feature2 = tf.SparseTensor(indices=edge_feature_index2,values=edge_feature_value2,\n",
    "                                       dense_shape=[num_edges2,num_features])\n",
    "        edge_feature_index3_init = tf.placeholder(dtype=tf.int64,shape=edge_feature_index30.shape)\n",
    "        edge_feature_index3 = tf.Variable(edge_feature_index3_init,trainable=False)\n",
    "        edge_feature_value3_init = tf.placeholder(dtype=tf.float32,shape=edge_feature_value30.shape)\n",
    "        edge_feature_value3 = tf.Variable(edge_feature_value3_init,trainable=False)\n",
    "        edge_feature3 = tf.SparseTensor(indices=edge_feature_index3,values=edge_feature_value3,\n",
    "                                       dense_shape=[num_edges3,num_features])\n",
    "        edge_feature_index4_init = tf.placeholder(dtype=tf.int64,shape=edge_feature_index40.shape)\n",
    "        edge_feature_index4 = tf.Variable(edge_feature_index4_init,trainable=False)\n",
    "        edge_feature_value4_init = tf.placeholder(dtype=tf.float32,shape=edge_feature_value40.shape)\n",
    "        edge_feature_value4 = tf.Variable(edge_feature_value4_init,trainable=False)\n",
    "        edge_feature4 = tf.SparseTensor(indices=edge_feature_index4,values=edge_feature_value4,\n",
    "                                       dense_shape=[num_edges4,num_features])\n",
    "        edge_feature_index5_init = tf.placeholder(dtype=tf.int64,shape=edge_feature_index50.shape)\n",
    "        edge_feature_index5 = tf.Variable(edge_feature_index5_init,trainable=False)\n",
    "        edge_feature_value5_init = tf.placeholder(dtype=tf.float32,shape=edge_feature_value50.shape)\n",
    "        edge_feature_value5 = tf.Variable(edge_feature_value5_init,trainable=False)\n",
    "        edge_feature5 = tf.SparseTensor(indices=edge_feature_index5,values=edge_feature_value5,\n",
    "                                       dense_shape=[num_edges5,num_features])\n",
    "        \n",
    "    # Create label_prop_matrix\n",
    "    if pattern_activation == 3:# Pattern C\n",
    "        if divide_calc == 0:\n",
    "            middle = tf.sigmoid(tf.sparse_tensor_dense_matmul(edge_feature,alpha1) + beta1)\n",
    "            edge_weight_data = tf.sigmoid(tf.matmul(middle,alpha2) + beta2)\n",
    "        else:\n",
    "            middle1 = tf.sigmoid(tf.sparse_tensor_dense_matmul(edge_feature1,alpha1) + beta1)\n",
    "            edge_weight_data1 = tf.sigmoid(tf.matmul(middle1,alpha2) + beta2)       \n",
    "            middle2 = tf.sigmoid(tf.sparse_tensor_dense_matmul(edge_feature2,alpha1) + beta1)\n",
    "            edge_weight_data2 = tf.sigmoid(tf.matmul(middle2,alpha2) + beta2)\n",
    "            middle3 = tf.sigmoid(tf.sparse_tensor_dense_matmul(edge_feature3,alpha1) + beta1)\n",
    "            edge_weight_data3 = tf.sigmoid(tf.matmul(middle3,alpha2) + beta2)    \n",
    "            middle4 = tf.sigmoid(tf.sparse_tensor_dense_matmul(edge_feature4,alpha1) + beta1)\n",
    "            edge_weight_data4 = tf.sigmoid(tf.matmul(middle4,alpha2) + beta2)   \n",
    "            middle5 = tf.sigmoid(tf.sparse_tensor_dense_matmul(edge_feature5,alpha1) + beta1)\n",
    "            edge_weight_data5 = tf.sigmoid(tf.matmul(middle5,alpha2) + beta2)   \n",
    "            edge_weight_data = tf.concat([edge_weight_data1, edge_weight_data2,edge_weight_data3,\n",
    "                      edge_weight_data4,edge_weight_data5], 0)\n",
    "    elif pattern_activation == 4:# Pattern D\n",
    "        middle = tf.nn.relu(tf.sparse_tensor_dense_matmul(edge_feature,alpha1) + beta1)\n",
    "        edge_weight_data = tf.sigmoid(tf.matmul(middle,alpha2) + beta2) \n",
    "    elif pattern_activation == 1:# Pattern A\n",
    "        temp = tf.sparse_tensor_dense_matmul(edge_feature,alpha)\n",
    "        edge_weight_data = tf.sigmoid(temp)\n",
    "    elif pattern_activation == 2:# Pattern B\n",
    "        temp = tf.sparse_tensor_dense_matmul(edge_feature,alpha)\n",
    "        edge_weight_data = tf.sigmoid(temp + beta)     \n",
    "    edge_weight_data = tf.reshape(edge_weight_data,[-1])\n",
    "    label_prop_matrix = tf.SparseTensor(indices=label_prop_row_col,values=edge_weight_data,\n",
    "                        dense_shape=tf.constant(core_matrix_shape,tf.int64))\n",
    "    if normalize==1:\n",
    "        b = tf.sparse_reduce_sum(label_prop_matrix, axis=1)\n",
    "        label_prop_matrix2 = label_prop_matrix / tf.reshape(b, (-1, 1))\n",
    "    # Inverse A : in line with MITPress-SemiSupervised Learning Label Propagation\n",
    "    A_inv = tf.SparseTensor(indices=label_prop_diagonal,values=label_prop_inverse_train,\n",
    "                            dense_shape=tf.constant(core_matrix_shape,tf.int64))\n",
    "    if initialize == 1:\n",
    "        f = 0*np.random.uniform(bound_left,bound_right,len(y_init_train))\n",
    "    else:\n",
    "        f = np.random.uniform(bound_left,bound_right,len(y_init_train))\n",
    "    #f = y_init_train\n",
    "    f = f.astype(\"float32\")\n",
    "    f = tf.reshape(f,[len(y_init_train),-1])\n",
    "    for i in range(inner_iteration):\n",
    "        f0 = f\n",
    "        if normalize == 1:\n",
    "            tempB = y_init_train + mu * tf.sparse_tensor_dense_matmul(label_prop_matrix2,f0)\n",
    "        else:\n",
    "            tempB = y_init_train + mu * tf.sparse_tensor_dense_matmul(label_prop_matrix,f0)\n",
    "        tempB = tf.reshape(tempB,(len(y_full),-1)) \n",
    "        f = tf.sparse_tensor_dense_matmul(A_inv,tempB,adjoint_a=False,adjoint_b=False,name=None)\n",
    "    # Retreive test predict\n",
    "    fpre = tf.gather_nd(f,eval_indices_train)\n",
    "    ytrue = y_full[eval_indices_train_list,0]\n",
    "    \n",
    "    # Regularize\n",
    "    if (pattern_activation == 3) | (pattern_activation == 4):\n",
    "        penalty = 0.0#lambda_reg * tf.reduce_sum(tf.abs(alpha1))/float(num_features)\n",
    "    else:\n",
    "        penalty = 0.0#lambda_reg * tf.reduce_sum(tf.abs(alpha))/float(num_features)\n",
    "    # loss\n",
    "    if pattern_loss == 1:\n",
    "        loss = tf.losses.mean_squared_error(fpre,ytrue)# + penalty\n",
    "    else:\n",
    "        pass\n",
    "        #loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_full[eval_indices_train_list,0],\n",
    "        #            logits=(-fpre))/float(len(eval_indices_train_list)) + penalty\n",
    "        \n",
    "    # Optimizer\n",
    "    my_opt = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "    train_step = my_opt.minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    if (pattern_activation == 3) | (pattern_activation==4):\n",
    "        if divide_calc==0:\n",
    "            sess.run(init,feed_dict={alpha1_init:alpha10,beta1_init:beta10,alpha2_init:alpha20,\n",
    "                beta2_init:beta20,\n",
    "                edge_feature_index_init:edge_feature_index0,\n",
    "                edge_feature_value_init:edge_feature_value0})\n",
    "        else:      \n",
    "            sess.run(init,feed_dict={alpha1_init:alpha10,beta1_init:beta10,alpha2_init:alpha20,\n",
    "                beta2_init:beta20,\n",
    "                edge_feature_index1_init:edge_feature_index10,\n",
    "                edge_feature_value1_init:edge_feature_value10,\n",
    "                edge_feature_index2_init:edge_feature_index20,\n",
    "                edge_feature_value2_init:edge_feature_value20,\n",
    "                edge_feature_index3_init:edge_feature_index30,\n",
    "                edge_feature_value3_init:edge_feature_value30,\n",
    "                edge_feature_index4_init:edge_feature_index40,\n",
    "                edge_feature_value4_init:edge_feature_value40,\n",
    "                edge_feature_index5_init:edge_feature_index50,\n",
    "                edge_feature_value5_init:edge_feature_value50})\n",
    "    elif pattern_activation == 2:    \n",
    "        sess.run(init,feed_dict={alpha_init:alpha0,beta_init:beta0,\n",
    "            edge_feature_index_init:edge_feature_index0,\n",
    "            edge_feature_value_init:edge_feature_value0})\n",
    "    else:\n",
    "        sess.run(init,feed_dict={alpha_init:alpha0,\n",
    "            edge_feature_index_init:edge_feature_index0,edge_feature_value_init:edge_feature_value0})\n",
    "\n",
    "    #_ = sess.run([train_step],feed_dict=None)\n",
    "    #oloss,oalpha,obeta,ofpre,otest = sess.run([loss,alpha,beta,fpre,edge_weight_data],feed_dict=None)\n",
    "    #otest = sess.run([edge_weight_data],feed_dict=None)\n",
    "\n",
    "    # Clear loss tracker\n",
    "    track_loss = []\n",
    "    oloss = sess.run([loss],feed_dict=None)\n",
    "    track_loss.append(oloss)\n",
    "    for itr in range(total_iteration):\n",
    "        _ = sess.run([train_step],feed_dict=None)\n",
    "        if (pattern_activation == 3) | (pattern_activation == 4):\n",
    "            oloss,oalpha1,obeta1,oalpha2,obeta2,ofpre = sess.run([loss,alpha1,beta1,alpha2,beta2,fpre])\n",
    "        elif(pattern_activation == 2):\n",
    "            oloss,oalpha,obeta,ofpre = sess.run([loss,alpha,beta,fpre],feed_dict=None)  \n",
    "        else:\n",
    "            oloss,oalpha,ofpre = sess.run([loss,alpha,fpre],feed_dict=None)   \n",
    "        if (itr % 100 == 0) | (itr == 0):\n",
    "            track_loss.append(oloss)\n",
    "            #print(\"Iteration: \" + str(itr) + \" Loss: \" + str(oloss))\n",
    "        if (itr % total_iteration == 0) | (itr == 0):\n",
    "            print(\"Iteration: \" + str(itr) + \" Loss: \" + str(oloss)) \n",
    "    \n",
    "    # EVALUATION #\n",
    "    ops.reset_default_graph()\n",
    "    sess =  tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True))\n",
    "    if (pattern_activation == 3) | (pattern_activation == 4):\n",
    "        alpha1_init = tf.placeholder(shape=alpha10.shape,dtype=tf.float32)\n",
    "        alpha1 = tf.Variable(alpha1_init)\n",
    "        alpha1 = tf.reshape(alpha1,[num_features,-1])\n",
    "        beta1_init = tf.placeholder(shape=beta10.shape,dtype=tf.float32)\n",
    "        beta1 = tf.Variable(beta1_init)\n",
    "        beta1 = tf.reshape(beta1,[-1,reduce_dimension])\n",
    "        alpha2_init = tf.placeholder(shape=alpha20.shape,dtype=tf.float32)\n",
    "        alpha2 = tf.Variable(alpha2_init)\n",
    "        alpha2 = tf.reshape(alpha2,[reduce_dimension,-1])\n",
    "        beta2_init = tf.placeholder(shape=beta20.shape,dtype=tf.float32)\n",
    "        beta2 = tf.Variable(beta2_init)\n",
    "    elif (pattern_activation == 2):\n",
    "        alpha_init = tf.placeholder(shape=alpha0.shape,dtype=tf.float32)\n",
    "        alpha = tf.Variable(alpha_init)\n",
    "        alpha = tf.reshape(alpha,[num_features,-1])\n",
    "        beta_init = tf.placeholder(shape=beta0.shape,dtype=tf.float32)\n",
    "        beta = tf.Variable(beta_init)\n",
    "    else:\n",
    "        alpha_init = tf.placeholder(shape=alpha0.shape,dtype=tf.float32)\n",
    "        alpha = tf.Variable(alpha_init)    \n",
    "        alpha = tf.reshape(alpha,[num_features,-1])\n",
    "\n",
    "    if divide_calc == 0:\n",
    "        edge_feature_index_init = tf.placeholder(dtype=tf.int64,shape=edge_feature_index0.shape)\n",
    "        edge_feature_index = tf.Variable(edge_feature_index_init,trainable=False)\n",
    "        edge_feature_value_init = tf.placeholder(dtype=tf.float32,shape=edge_feature_value0.shape)\n",
    "        edge_feature_value = tf.Variable(edge_feature_value_init,trainable=False)\n",
    "        edge_feature = tf.SparseTensor(indices=edge_feature_index,values=edge_feature_value,\n",
    "                                       dense_shape=[num_edges,num_features])\n",
    "    else:\n",
    "        edge_feature_index1_init = tf.placeholder(dtype=tf.int64,shape=edge_feature_index10.shape)\n",
    "        edge_feature_index1 = tf.Variable(edge_feature_index1_init,trainable=False)\n",
    "        edge_feature_value1_init = tf.placeholder(dtype=tf.float32,shape=edge_feature_value10.shape)\n",
    "        edge_feature_value1 = tf.Variable(edge_feature_value1_init,trainable=False)\n",
    "        edge_feature1 = tf.SparseTensor(indices=edge_feature_index1,values=edge_feature_value1,\n",
    "                                       dense_shape=[num_edges1,num_features])\n",
    "        edge_feature_index2_init = tf.placeholder(dtype=tf.int64,shape=edge_feature_index20.shape)\n",
    "        edge_feature_index2 = tf.Variable(edge_feature_index2_init,trainable=False)\n",
    "        edge_feature_value2_init = tf.placeholder(dtype=tf.float32,shape=edge_feature_value20.shape)\n",
    "        edge_feature_value2 = tf.Variable(edge_feature_value2_init,trainable=False)\n",
    "        edge_feature2 = tf.SparseTensor(indices=edge_feature_index2,values=edge_feature_value2,\n",
    "                                       dense_shape=[num_edges2,num_features])\n",
    "        edge_feature_index3_init = tf.placeholder(dtype=tf.int64,shape=edge_feature_index30.shape)\n",
    "        edge_feature_index3 = tf.Variable(edge_feature_index3_init,trainable=False)\n",
    "        edge_feature_value3_init = tf.placeholder(dtype=tf.float32,shape=edge_feature_value30.shape)\n",
    "        edge_feature_value3 = tf.Variable(edge_feature_value3_init,trainable=False)\n",
    "        edge_feature3 = tf.SparseTensor(indices=edge_feature_index3,values=edge_feature_value3,\n",
    "                                       dense_shape=[num_edges3,num_features])\n",
    "        edge_feature_index4_init = tf.placeholder(dtype=tf.int64,shape=edge_feature_index40.shape)\n",
    "        edge_feature_index4 = tf.Variable(edge_feature_index4_init,trainable=False)\n",
    "        edge_feature_value4_init = tf.placeholder(dtype=tf.float32,shape=edge_feature_value40.shape)\n",
    "        edge_feature_value4 = tf.Variable(edge_feature_value4_init,trainable=False)\n",
    "        edge_feature4 = tf.SparseTensor(indices=edge_feature_index4,values=edge_feature_value4,\n",
    "                                       dense_shape=[num_edges4,num_features])\n",
    "        edge_feature_index5_init = tf.placeholder(dtype=tf.int64,shape=edge_feature_index50.shape)\n",
    "        edge_feature_index5 = tf.Variable(edge_feature_index5_init,trainable=False)\n",
    "        edge_feature_value5_init = tf.placeholder(dtype=tf.float32,shape=edge_feature_value50.shape)\n",
    "        edge_feature_value5 = tf.Variable(edge_feature_value5_init,trainable=False)\n",
    "        edge_feature5 = tf.SparseTensor(indices=edge_feature_index5,values=edge_feature_value5,\n",
    "                                       dense_shape=[num_edges5,num_features])\n",
    "        \n",
    "    # Create label_prop_matrix\n",
    "    if pattern_activation == 3:# Pattern C\n",
    "        if divide_calc == 0:\n",
    "            middle = tf.sigmoid(tf.sparse_tensor_dense_matmul(edge_feature,alpha1) + beta1)\n",
    "            edge_weight_data = tf.sigmoid(tf.matmul(middle,alpha2) + beta2)\n",
    "        else:\n",
    "            middle1 = tf.sigmoid(tf.sparse_tensor_dense_matmul(edge_feature1,alpha1) + beta1)\n",
    "            edge_weight_data1 = tf.sigmoid(tf.matmul(middle1,alpha2) + beta2)       \n",
    "            middle2 = tf.sigmoid(tf.sparse_tensor_dense_matmul(edge_feature2,alpha1) + beta1)\n",
    "            edge_weight_data2 = tf.sigmoid(tf.matmul(middle2,alpha2) + beta2)\n",
    "            middle3 = tf.sigmoid(tf.sparse_tensor_dense_matmul(edge_feature3,alpha1) + beta1)\n",
    "            edge_weight_data3 = tf.sigmoid(tf.matmul(middle3,alpha2) + beta2)    \n",
    "            middle4 = tf.sigmoid(tf.sparse_tensor_dense_matmul(edge_feature4,alpha1) + beta1)\n",
    "            edge_weight_data4 = tf.sigmoid(tf.matmul(middle4,alpha2) + beta2)   \n",
    "            middle5 = tf.sigmoid(tf.sparse_tensor_dense_matmul(edge_feature5,alpha1) + beta1)\n",
    "            edge_weight_data5 = tf.sigmoid(tf.matmul(middle5,alpha2) + beta2)   \n",
    "            edge_weight_data = tf.concat([edge_weight_data1, edge_weight_data2,edge_weight_data3,\n",
    "                      edge_weight_data4,edge_weight_data5], 0)\n",
    "    elif pattern_activation == 4:# Pattern D\n",
    "        middle = tf.nn.relu(tf.sparse_tensor_dense_matmul(edge_feature,alpha1) + beta1)\n",
    "        edge_weight_data = tf.sigmoid(tf.matmul(middle,alpha2) + beta2) \n",
    "    elif pattern_activation == 1:# Pattern A\n",
    "        temp = tf.sparse_tensor_dense_matmul(edge_feature,alpha)\n",
    "        edge_weight_data = tf.sigmoid(temp)\n",
    "    elif pattern_activation == 2:# Pattern B\n",
    "        temp = tf.sparse_tensor_dense_matmul(edge_feature,alpha)\n",
    "        edge_weight_data = tf.sigmoid(temp + beta)     \n",
    "    edge_weight_data = tf.reshape(edge_weight_data,[-1])\n",
    "    \n",
    "    label_prop_matrix = tf.SparseTensor(indices=label_prop_row_col,values=edge_weight_data,\n",
    "                        dense_shape=tf.constant(core_matrix_shape,tf.int64))    \n",
    "\n",
    "    if normalize==1:\n",
    "        b = tf.sparse_reduce_sum(label_prop_matrix, axis=1)\n",
    "        label_prop_matrix2 = label_prop_matrix / tf.reshape(b, (-1, 1))\n",
    "    \n",
    "    # Inverse A : in line with MITPress-SemiSupervised Learning Label Propagation\n",
    "    A_inv = tf.SparseTensor(indices=label_prop_diagonal,values=label_prop_inverse_test,\n",
    "                            dense_shape=tf.constant(core_matrix_shape,tf.int64))\n",
    "    \n",
    "    if initialize == 1:\n",
    "        f = 0*np.random.uniform(bound_left,bound_right,len(y_init_test))\n",
    "    else:\n",
    "        f = np.random.uniform(bound_left,bound_right,len(y_init_test))\n",
    "    #f = y_init_test\n",
    "    #f = 0*np.random.uniform(bound_left,bound_right,len(y_init_test))\n",
    "    f = f.astype(\"float32\")\n",
    "    f = tf.reshape(f,[len(y_init_test),-1])\n",
    "    for i in range(inner_iteration):\n",
    "        f0 = f\n",
    "        if normalize == 1:\n",
    "            tempB = y_init_test + mu * tf.sparse_tensor_dense_matmul(label_prop_matrix2,f0)\n",
    "        else:\n",
    "            tempB = y_init_test + mu * tf.sparse_tensor_dense_matmul(label_prop_matrix,f0)\n",
    "        tempB = tf.reshape(tempB,(len(y_full),-1)) \n",
    "        f = tf.sparse_tensor_dense_matmul(A_inv,tempB,adjoint_a=False,adjoint_b=False,name=None)\n",
    "    # Retreive test predict\n",
    "    fpre = tf.gather_nd(f,eval_indices_test)\n",
    "    # Regularize\n",
    "    if (pattern_activation == 3) | (pattern_activation == 4):\n",
    "        penalty = lambda_reg * tf.reduce_sum(tf.abs(alpha1))/float(num_features)\n",
    "    else:\n",
    "        penalty = lambda_reg * tf.reduce_sum(tf.abs(alpha))/float(num_features)\n",
    "    # loss\n",
    "    if pattern_loss == 1:\n",
    "        loss = tf.losses.mean_squared_error(fpre,y_full[eval_indices_test_list,0]) + penalty\n",
    "    else:\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_full[eval_indices_test_list,0],\n",
    "                    logits=(-fpre))/float(len(eval_indices_test_list)) + penalty\n",
    "\n",
    "    # Optimizer\n",
    "    my_opt = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "    train_step = my_opt.minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    if (pattern_activation == 3) | (pattern_activation==4):\n",
    "        if divide_calc==0:\n",
    "            sess.run(init,feed_dict={alpha1_init:alpha10,beta1_init:beta10,alpha2_init:alpha20,\n",
    "                beta2_init:beta20,\n",
    "                edge_feature_index_init:edge_feature_index0,\n",
    "                edge_feature_value_init:edge_feature_value0})\n",
    "        else:      \n",
    "            sess.run(init,feed_dict={alpha1_init:alpha10,beta1_init:beta10,alpha2_init:alpha20,\n",
    "                beta2_init:beta20,\n",
    "                edge_feature_index1_init:edge_feature_index10,\n",
    "                edge_feature_value1_init:edge_feature_value10,\n",
    "                edge_feature_index2_init:edge_feature_index20,\n",
    "                edge_feature_value2_init:edge_feature_value20,\n",
    "                edge_feature_index3_init:edge_feature_index30,\n",
    "                edge_feature_value3_init:edge_feature_value30,\n",
    "                edge_feature_index4_init:edge_feature_index40,\n",
    "                edge_feature_value4_init:edge_feature_value40,\n",
    "                edge_feature_index5_init:edge_feature_index50,\n",
    "                edge_feature_value5_init:edge_feature_value50})\n",
    "    elif pattern_activation == 2:    \n",
    "        sess.run(init,feed_dict={alpha_init:alpha0,beta_init:beta0,\n",
    "            edge_feature_index_init:edge_feature_index0,\n",
    "            edge_feature_value_init:edge_feature_value0})\n",
    "    else:\n",
    "        sess.run(init,feed_dict={alpha_init:alpha0,\n",
    "            edge_feature_index_init:edge_feature_index0,edge_feature_value_init:edge_feature_value0})\n",
    "    \n",
    "    for itr in range(1):\n",
    "        if (pattern_activation == 3) | (pattern_activation == 4):\n",
    "            oloss,oalpha1,obeta1,oalpha2,obeta2,ofpre = sess.run([loss,alpha1,beta1,alpha2,beta2,fpre])\n",
    "        elif(pattern_activation == 2):\n",
    "            oloss,oalpha,obeta,ofpre = sess.run([loss,alpha,beta,fpre],feed_dict=None)  \n",
    "        else:\n",
    "            oloss,oalpha,ofpre = sess.run([loss,alpha,fpre],feed_dict=None)   \n",
    "\n",
    "    # Evaluate\n",
    "    ypre = ofpre\n",
    "    ylab= y_full[eval_indices_test_list]\n",
    "    # Calculate\n",
    "    random_guess = np.sum(ylab)/len(ylab)\n",
    "    precision, recall, thresholds = precision_recall_curve(ylab,ypre)\n",
    "    area = auc(recall, precision)\n",
    "    print(\"Random Prediction: \" + str(random_guess) + \\\n",
    "              \" AP: \" + str(average_precision_score(ylab, ypre, average='weighted')) + \\\n",
    "              \" ROC: \" + str(roc_auc_score(ylab, ypre)))\n",
    "\n",
    "    file_pdf = \"figure_20170201/\" + train_test_split_time + \"_path_\" + \\\n",
    "            use_label + \".pdf\"\n",
    "    params = {\n",
    "       'axes.labelsize': 14,\n",
    "       'font.size': 14,\n",
    "       'legend.fontsize': 14,\n",
    "       'xtick.labelsize': 14,\n",
    "       'ytick.labelsize': 14,\n",
    "       'text.usetex': False,\n",
    "       'figure.figsize': [6, 4]\n",
    "    }\n",
    "    plt.rcParams.update(params)\n",
    "    plt.xlabel('Score')\n",
    "    plt.ylabel('True Label')\n",
    "    yint = range(0, 2)\n",
    "    plt.yticks(yint)\n",
    "    plt.plot(ypre,ylab,marker=\"+\",markersize=10,\n",
    "             markeredgewidth=1.5,linewidth=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_pdf)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LP-mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bound_left = 0\n",
    "#bound_right = 1\n",
    "#initialize = 1\n",
    "#temp_date0 = datetime.datetime.strptime(\"2017-02-01\",\"%Y-%m-%d\")\n",
    "#lag_days = 31\n",
    "#feature_dim = 50\n",
    "#use_raw = 1\n",
    "#one_minus_one = 0\n",
    "#correct_norm = 0\n",
    "#epsilon = 0.0\n",
    "#verbose = 1\n",
    "#use_label0 = \"Association\"\n",
    "#keisu = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultLP(use_label0,threshold_ijyou,total_iteration,temp_date0,\n",
    "           normalize,lag_days):\n",
    "\n",
    "    # START COMMON: 20180920 #\n",
    "    use_label = re.sub(\"/\",\"-\",use_label0)\n",
    "    Edges.InitializeSparseCoreMatrix(1)\n",
    "    temp_date1 = temp_date0 - datetime.timedelta(days=lag_days)\n",
    "    temp_date2 = temp_date0 + datetime.timedelta(days=50000)\n",
    "    train_test_split_time = datetime.datetime.strftime(temp_date0,\"%Y-%m-%d\")\n",
    "    train_deve_split_time = datetime.datetime.strftime(temp_date1,\"%Y-%m-%d\")\n",
    "    test_end_split_time = datetime.datetime.strftime(temp_date2,\"%Y-%m-%d\")\n",
    "    print(\"train test:\" + str(train_test_split_time) + \",train deve:\" + str(train_deve_split_time))\n",
    "    file_object = header + \"_\" + use_label + \".pkl\"\n",
    "    with open(file_object, mode='rb') as f:\n",
    "        objects = pickle.load(f)\n",
    "    train_positive0 = objects[0]\n",
    "    train_positive_time_list = objects[1]\n",
    "    test_positive0  = objects[2]\n",
    "    test_positive_time_list = objects[3]\n",
    "    print(use_label + \" Num Train: \" + str(len(train_positive0)) + \" Num Test: \" + str(len(test_positive0)))\n",
    "    Edges.ClearTrainPositiveTime()\n",
    "    for i in range(len(train_positive_time_list)):\n",
    "        Edges.train_positive_time.append(train_positive_time_list[i])\n",
    "    # END COMMON: 20180920 #\n",
    "    \n",
    "    Edges.CreateMultMatrix(train_positive0,\n",
    "                train_deve_split_time,test_positive0,threshold_ijyou,\n",
    "                max_depth,reduce_dimension,mu,epsilon)\n",
    "    label_prop_row_col = []\n",
    "    for i in range(len(Edges.label_prop_row)):\n",
    "        label_prop_row_col.append([np.int64(Edges.label_prop_row[i]),np.int64(Edges.label_prop_col[i])])\n",
    "    \n",
    "    # START COMMON: 20180920 #\n",
    "    y_init_train = Edges.y_init_train\n",
    "    y_init_train = np.reshape(y_init_train,[-1,1])\n",
    "    y_init_train = y_init_train.astype(\"float32\")\n",
    "    y_init_test = Edges.y_init_test\n",
    "    y_init_test = np.reshape(y_init_test,[-1,1])\n",
    "    y_init_test = y_init_test.astype(\"float32\")\n",
    "    y_full = Edges.y_full\n",
    "    y_full = np.reshape(y_full,[-1,1])\n",
    "    y_full = y_full.astype(\"float32\")\n",
    "    print(str(np.sum(y_init_train)) + \",\" + str(np.sum(y_init_test)) + \",\" + str(np.sum(y_full)) )\n",
    "    #if one_minus_one == 1:\n",
    "    #    y_init_train = 2*y_init_train - 1\n",
    "    #    y_init_test = 2*y_init_test - 1\n",
    "    #    y_full = 2*y_full - 1\n",
    "    eval_indices_train,eval_indices_train_list = ([] for _ in range(2))\n",
    "    for i in range(len(Edges.eval_indices_train)):\n",
    "        eval_indices_train_list.append(np.int64(Edges.eval_indices_train[i]))\n",
    "        eval_indices_train.append([np.int64(Edges.eval_indices_train[i]),0])\n",
    "    eval_indices_train = np.asarray(eval_indices_train,np.int64)\n",
    "    eval_indices_test,eval_indices_test_list = ([] for _ in range(2))\n",
    "    for i in range(len(Edges.eval_indices_test)):\n",
    "        eval_indices_test_list.append(np.int64(Edges.eval_indices_test[i]))\n",
    "        eval_indices_test.append([np.int64(Edges.eval_indices_test[i]),0])\n",
    "    eval_indices_test = np.asarray(eval_indices_test,np.int64)\n",
    "    print(str(len(eval_indices_train_list)) + \",\" + str(len(eval_indices_test_list)))\n",
    "    label_prop_diagonal = []\n",
    "    for i in range(len(y_full)):\n",
    "        label_prop_diagonal.append([np.int64(i),np.int64(i)])\n",
    "    label_prop_diagonal = np.asarray(label_prop_diagonal,np.int64)\n",
    "    label_prop_inverse_train = Edges.label_prop_inverse_train\n",
    "    label_prop_inverse_train = np.reshape(label_prop_inverse_train,-1)\n",
    "    label_prop_inverse_train = label_prop_inverse_train.astype(\"float32\")\n",
    "    label_prop_inverse_test = Edges.label_prop_inverse_test\n",
    "    label_prop_inverse_test = np.reshape(label_prop_inverse_test ,-1)\n",
    "    label_prop_inverse_test = label_prop_inverse_test.astype(\"float32\")\n",
    "    core_matrix_shape = [len(y_full),len(y_full)]\n",
    "    core_matrix_shape = np.array(core_matrix_shape, dtype=np.int64)\n",
    "    #num_edges = edge_feature0.shape[0]\n",
    "    #num_features = edge_feature0.shape[1]  \n",
    "    # END COMMON: 20180920 #\n",
    "    \n",
    "    all_label = [   \n",
    "    \"Product/Service\", # 32965\n",
    "    \"Regulatory\", # 26867\n",
    "    \"Financial\", # 22712\n",
    "    \"Fraud\", # 16410\n",
    "    \"Workforce\", # 13016\n",
    "    \"Management\", # 12467\n",
    "    \"Anti-Competitive\", # 10410\n",
    "    \"Information\", # 8809\n",
    "    \"Workplace\", # 8216\n",
    "    \"Discrimination/Workforce\", # 7141\n",
    "    \"Environmental\", # 5645\n",
    "    \"Ownership\", # 4727\n",
    "    \"Production/Supply\", # 4086\n",
    "    \"Corruption\", # 3985\n",
    "    \"Human\", # 556\n",
    "    \"Sanctions\", # 252\n",
    "    \"Association\" # 243\n",
    "    ]\n",
    "    Edges.ClearLabels()\n",
    "    for i in all_label:\n",
    "        Edges.use_label_vector.append(i)\n",
    "    Edges.CreateObjectsMult(file_dataframe,use_label0,\n",
    "            train_test_split_time,test_end_split_time,start_time)\n",
    "    label_matrix_mult_train = Edges.label_matrix_mult_train\n",
    "    label_matrix_mult_train = label_matrix_mult_train.astype(\"float32\")\n",
    "    label_matrix_mult_test = Edges.label_matrix_mult_test\n",
    "    label_matrix_mult_test = label_matrix_mult_test.astype(\"float32\")\n",
    "\n",
    "    index = 0\n",
    "    for ii in range(len(all_label)):\n",
    "        if all_label[ii] == use_label0:\n",
    "            index = ii\n",
    "            break\n",
    "    if 1 == 0:# Sanity Check\n",
    "        for i in range(10):\n",
    "            row = Edges.update_indices[i,0]\n",
    "            col = Edges.update_indices[i,1]\n",
    "            print(label_matrix_mult_train[row,col])\n",
    "            \n",
    "    num_iteration = 20\n",
    "    num_samp = label_matrix_mult_train.shape[0]\n",
    "    Edges.MultLP(num_iteration,\n",
    "             0.01/num_samp,\n",
    "             0.02,0.4,1)\n",
    "    \n",
    "    #for i in range(1,num_iteration):\n",
    "    #    Ypre = Edges.GetResult(i)\n",
    "    #    if i!=1:\n",
    "    #        print(np.linalg.norm(Ypre - Ymoto))\n",
    "    #    Ymoto = Ypre    \n",
    "\n",
    "    #for i in range(2,num_iteration):\n",
    "    Ypre = Edges.GetResult(num_iteration-1)\n",
    "    # Evaluate\n",
    "    ypre = Ypre[eval_indices_test_list,index]\n",
    "    ylab= y_full[eval_indices_test_list]\n",
    "    # Calculate\n",
    "    random_guess = np.sum(ylab)/len(ylab)\n",
    "    precision, recall, thresholds = precision_recall_curve(ylab,ypre)\n",
    "    area = auc(recall, precision)\n",
    "    print(\"Random Prediction: \" + str(random_guess) + \\\n",
    "              \" AP: \" + str(average_precision_score(ylab, ypre, \n",
    "                    average='weighted')) + \\\n",
    "              \" ROC: \" + str(roc_auc_score(ylab, ypre)))\n",
    "    file_pdf = \"figure_20170201/\" + train_test_split_time + \"_multlp_\" + \\\n",
    "            use_label + \".pdf\"\n",
    "    params = {\n",
    "       'axes.labelsize': 14,\n",
    "       'font.size': 14,\n",
    "       'legend.fontsize': 14,\n",
    "       'xtick.labelsize': 14,\n",
    "       'ytick.labelsize': 14,\n",
    "       'text.usetex': False,\n",
    "       'figure.figsize': [6, 4]\n",
    "    }\n",
    "    plt.rcParams.update(params)\n",
    "    plt.xlabel('Score')\n",
    "    plt.ylabel('True Label')\n",
    "    yint = range(0, 2)\n",
    "    plt.yticks(yint)\n",
    "    plt.plot(ypre,ylab,marker=\"+\",markersize=10,\n",
    "             markeredgewidth=1.5,linewidth=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_pdf)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SELECT ####\n",
    "temp = [   \n",
    "\"Product/Service\", \n",
    "\"Regulatory\",\n",
    "\"Financial\", \n",
    "\"Fraud\",\n",
    "\"Workforce\", \n",
    "\"Management\", \n",
    "\"Anti-Competitive\",\n",
    "\"Information\", \n",
    "\"Workplace\",\n",
    "\"Discrimination/Workforce\",\n",
    "\"Environmental\",\n",
    "\"Ownership\", \n",
    "\"Production/Supply\",\n",
    "\"Corruption\",\n",
    "\"Human\", \n",
    "\"Sanctions\", \n",
    "\"Association\" \n",
    "]\n",
    "\n",
    "lag_days_dict = {}\n",
    "for i in range(len(temp)):\n",
    "    if i < 14:\n",
    "        lag_days_dict.update({temp[i]:31})\n",
    "    else:\n",
    "        lag_days_dict.update({temp[i]:182})\n",
    "\n",
    "parameters = []\n",
    "for j in range(1,2):\n",
    "    for i in range(len(temp)):\n",
    "        use_label0 = temp[i]\n",
    "        threshold_ijyou = j\n",
    "        total_iteration = 10000\n",
    "        normalize = 0\n",
    "        ## BE CAREFUL HERE #\n",
    "        parameters.append([use_label0,threshold_ijyou,total_iteration,\n",
    "            datetime.datetime.strptime(\"2017-02-01\",\"%Y-%m-%d\"),\n",
    "            normalize,lag_days_dict[use_label0]\n",
    "        ])\n",
    "print(len(parameters))\n",
    "print(train_test_split_time_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CUT = 10000\n",
    "for para in parameters:\n",
    "    ## DONT CHANGE ##\n",
    "    initialize = 1\n",
    "    one_minus_one = 0\n",
    "    pattern_activation = 3\n",
    "    zero_one = 1\n",
    "    feature_dim = 50\n",
    "    reduce_dim = 30\n",
    "    zero_one = 1\n",
    "    inner_iteration = 100\n",
    "    #################\n",
    "    \n",
    "    # Create Label\n",
    "    PrintNum(para[0],para[1],para[2],para[3],para[4],\n",
    "        pattern_activation,reduce_dim,initialize,CUT,zero_one,para[5],\n",
    "        feature_dim,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CUT = 10000\n",
    "for para in parameters:\n",
    "    ## DONT CHANGE ##\n",
    "    initialize = 1\n",
    "    one_minus_one = 0\n",
    "    pattern_activation = 3\n",
    "    zero_one = 1\n",
    "    feature_dim = 50\n",
    "    reduce_dim = 30\n",
    "    zero_one = 1\n",
    "    inner_iteration = 100\n",
    "    #################\n",
    "    \n",
    "    # Create Label\n",
    "    PrintNum(para[0],para[1],para[2],para[3],para[4],\n",
    "        pattern_activation,reduce_dim,initialize,CUT,zero_one,para[5],\n",
    "        feature_dim,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LP Fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUT = 10000\n",
    "for para in parameters:\n",
    "    ## DONT CHANGE ##\n",
    "    initialize = 1\n",
    "    one_minus_one = 0\n",
    "    pattern_activation = 1\n",
    "    zero_one = 1\n",
    "    feature_dim = 50\n",
    "    reduce_dim = 30\n",
    "    zero_one = 1\n",
    "    inner_iteration = 100\n",
    "    #################\n",
    "    \n",
    "    # Fixed LP\n",
    "    CoreFixed(para[0],para[1],para[2],para[3],para[4],initialize,para[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LP-core-relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUT = 10000\n",
    "for para in parameters:\n",
    "    ## DONT CHANGE ##\n",
    "    initialize = 1\n",
    "    one_minus_one = 0\n",
    "    pattern_activation = 3\n",
    "    zero_one = 1\n",
    "    feature_dim = 50\n",
    "    reduce_dim = 30\n",
    "    zero_one = 1\n",
    "    inner_iteration = 100\n",
    "    bound_left = 0\n",
    "    bound_right = 1\n",
    "    lambda_reg = 0\n",
    "    learning_rate = 0.1\n",
    "    pattern_loss = 1\n",
    "    verbose = 0\n",
    "    correct_norm = 0\n",
    "    keisu = 0.1\n",
    "    #################\n",
    "\n",
    "    # Only Relation Type\n",
    "    OneEdge(para[0],para[1],para[2],para[3],para[4],\n",
    "        pattern_activation,reduce_dim,initialize,CUT,zero_one,para[5],\n",
    "        one_minus_one,correct_norm,keisu,verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUT = 10000\n",
    "for para in parameters:\n",
    "    ## DONT CHANGE ##\n",
    "    initialize = 1\n",
    "    one_minus_one = 0\n",
    "    pattern_activation = 3\n",
    "    zero_one = 1\n",
    "    feature_dim = 50\n",
    "    reduce_dim = 30\n",
    "    zero_one = 1\n",
    "    inner_iteration = 100\n",
    "    bound_left = 0\n",
    "    bound_right = 1\n",
    "    lambda_reg = 0\n",
    "    learning_rate = 0.1\n",
    "    pattern_loss = 1\n",
    "    verbose = 0\n",
    "    #################\n",
    "\n",
    "    # HIN  NMF\n",
    "    OneBP2(para[0],para[1],para[2],para[3],para[4],\n",
    "        pattern_activation,reduce_dim,initialize,CUT,zero_one,para[5],\n",
    "        feature_dim,0,one_minus_one,0,0.0,0.1,verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUT = 10000\n",
    "for para in parameters:\n",
    "    ## DONT CHANGE ##\n",
    "    initialize = 1\n",
    "    one_minus_one = 0\n",
    "    pattern_activation = 3\n",
    "    zero_one = 1\n",
    "    feature_dim = 50\n",
    "    reduce_dim = 30\n",
    "    zero_one = 1\n",
    "    inner_iteration = 100\n",
    "    bound_left = 0\n",
    "    bound_right = 1\n",
    "    lambda_reg = 0\n",
    "    learning_rate = 0.1\n",
    "    pattern_loss = 1\n",
    "    verbose = 0\n",
    "    #################\n",
    "\n",
    "    # HIN RAW\n",
    "    OneBP2(para[0],para[1],para[2],para[3],para[4],pattern_activation,\n",
    "        reduce_dim,initialize,CUT,zero_one,para[5],\n",
    "        feature_dim,1,one_minus_one,0,0.0,0.1,verbose)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LP-mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUT = 10000\n",
    "for para in parameters:\n",
    "    ## DONT CHANGE ##\n",
    "    initialize = 1\n",
    "    one_minus_one = 0\n",
    "    pattern_activation = 3\n",
    "    zero_one = 1\n",
    "    feature_dim = 50\n",
    "    reduce_dim = 30\n",
    "    zero_one = 1\n",
    "    inner_iteration = 100\n",
    "    bound_left = 0\n",
    "    bound_right = 1\n",
    "    lambda_reg = 0\n",
    "    learning_rate = 0.1\n",
    "    pattern_loss = 1\n",
    "    verbose = 0\n",
    "    #################\n",
    "    \n",
    "    MultLP(para[0],para[1],para[2],para[3],para[4],para[5])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LP-Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SELECT ####\n",
    "temp = [   \n",
    "\"Management\", \n",
    "\"Anti-Competitive\",\n",
    "\"Information\", \n",
    "\"Workplace\",\n",
    "\"Discrimination/Workforce\",\n",
    "\"Environmental\",\n",
    "\"Ownership\", \n",
    "\"Production/Supply\",\n",
    "\"Corruption\",\n",
    "\"Human\", \n",
    "\"Sanctions\", \n",
    "\"Association\" \n",
    "]\n",
    "\n",
    "lag_days_dict = {}\n",
    "for i in range(len(temp)):\n",
    "    if i < 9:\n",
    "        lag_days_dict.update({temp[i]:31})\n",
    "    else:\n",
    "        lag_days_dict.update({temp[i]:182})\n",
    "\n",
    "parameters = []\n",
    "for j in range(1,2):\n",
    "    for i in range(len(temp)):\n",
    "        use_label0 = temp[i]\n",
    "        threshold_ijyou = j\n",
    "        total_iteration = 10000\n",
    "        normalize = 0\n",
    "        ## BE CAREFUL HERE #\n",
    "        parameters.append([use_label0,threshold_ijyou,total_iteration,\n",
    "            datetime.datetime.strptime(\"2017-02-01\",\"%Y-%m-%d\"),\n",
    "            normalize,lag_days_dict[use_label0]\n",
    "        ])\n",
    "print(len(parameters))\n",
    "print(train_test_split_time_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CUT = 10000\n",
    "for para in parameters:\n",
    "    ## DONT CHANGE ##\n",
    "    initialize = 1\n",
    "    one_minus_one = 0\n",
    "    pattern_activation = 3\n",
    "    zero_one = 1\n",
    "    feature_dim = 50\n",
    "    reduce_dim = 30\n",
    "    zero_one = 1\n",
    "    inner_iteration = 100\n",
    "    bound_left = 0\n",
    "    bound_right = 1\n",
    "    lambda_reg = 0\n",
    "    learning_rate = 0.1\n",
    "    pattern_loss = 1\n",
    "    verbose = 0\n",
    "    #################\n",
    "    \n",
    "    # Path Rank\n",
    "    Path2(para[0],para[1],para[2],para[3],para[4],\n",
    "          pattern_activation,reduce_dim,initialize,CUT,zero_one,para[5],0.1,\n",
    "          3000)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "630px",
   "left": "0px",
   "right": "878px",
   "top": "111px",
   "width": "94px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
